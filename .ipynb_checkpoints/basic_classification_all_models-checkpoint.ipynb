{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "#models:\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,\\\n",
    "GradientBoostingClassifier, VotingClassifier, BaggingClassifier, ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "##\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import cross_validate\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read all the datasets\n",
    "def read_all(template,start,end):\n",
    "    frames = [ pd.read_json(f).fillna(0) for f in [template.format(i) for i in range(start,end)] ]\n",
    "    X = pd.concat(frames, ignore_index = True,sort = True)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"datasets/dataset_finalized/dataset_{:02}.json\"\n",
    "df = read_all(template,0,18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 87141 entries, 0 to 87140\n",
      "Columns: 178 entries, Ak47_ct to t_leads\n",
      "dtypes: float64(11), int64(167)\n",
      "memory usage: 118.3 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "colwep = ['Ak47_ct', 'Ak47_t', 'Aug_ct', 'Aug_t', 'Awp_ct', 'Awp_t', 'C4_t', 'Cz75Auto_ct',\\\n",
    "          'Cz75Auto_t', 'Deagle_ct', 'Deagle_t', 'DecoyGrenade_ct', 'DecoyGrenade_t', 'Flashbang_ct',\\\n",
    "          'Flashbang_t', 'Glock_ct', 'Glock_t', 'HeGrenade_ct', 'HeGrenade_t', 'M4a4_ct', 'M4a4_t',\\\n",
    "          'MolotovIncendiaryGrenade_ct', 'MolotovIncendiaryGrenade_t', 'Mp9_ct', 'Mp9_t', 'P2000_ct',\\\n",
    "          'P2000_t', 'P250_ct', 'P250_t', 'Sg553_ct', 'Sg553_t', 'SmokeGrenade_ct', 'SmokeGrenade_t',\\\n",
    "          'UspS_ct', 'UspS_t','other_heavy_ct', 'other_heavy_t', 'other_pistols_ct', \\\n",
    "          'other_pistols_t', 'other_rifles_ct', 'other_rifles_t', 'other_smgs_ct', 'other_smgs_t']\n",
    "\n",
    "# colwep = ['Ak47_ct', 'Ak47_t', 'Aug_ct', 'Aug_t', 'Awp_ct', 'Awp_t', 'C4_t',\\\n",
    "#           'Deagle_ct', 'Deagle_t', 'Flashbang_ct',\\\n",
    "#           'Flashbang_t', 'Glock_ct', 'Glock_t', 'HeGrenade_ct', 'HeGrenade_t', 'M4a4_ct', 'M4a4_t',\\\n",
    "#           'MolotovIncendiaryGrenade_ct', 'MolotovIncendiaryGrenade_t', 'P2000_ct',\\\n",
    "#           'P2000_t', 'P250_ct', 'P250_t', 'Sg553_ct', 'Sg553_t', 'SmokeGrenade_ct', 'SmokeGrenade_t',\\\n",
    "#           'UspS_ct', 'UspS_t']\n",
    "\n",
    "colpla = ['alive_players_ct', 'alive_players_t', 'armor_ct1_Bin_Code', 'armor_ct2_Bin_Code',\\\n",
    "          'armor_ct3_Bin_Code', 'armor_ct4_Bin_Code', 'armor_ct5_Bin_Code', 'armor_ct_Bin_Code',\\\n",
    "          'armor_t1_Bin_Code', 'armor_t2_Bin_Code', 'armor_t3_Bin_Code', 'armor_t4_Bin_Code',\\\n",
    "          'armor_t5_Bin_Code', 'armor_t_Bin_Code','defuse_kit_ct1', 'defuse_kit_ct2',\\\n",
    "          'defuse_kit_ct3', 'defuse_kit_ct4', 'defuse_kit_ct5', 'has_helmet_ct1', 'has_helmet_ct2',\\\n",
    "          'has_helmet_ct3', 'has_helmet_ct4', 'has_helmet_ct5', 'has_helmet_t1', 'has_helmet_t2',\\\n",
    "          'has_helmet_t3', 'has_helmet_t4', 'has_helmet_t5', 'health_ct1_Bin_Code',\\\n",
    "          'health_ct2_Bin_Code', 'health_ct3_Bin_Code', 'health_ct4_Bin_Code', 'health_ct5_Bin_Code',\\\n",
    "          'health_t1_Bin_Code', 'health_t2_Bin_Code', 'health_t3_Bin_Code', 'health_t4_Bin_Code',\\\n",
    "          'health_t5_Bin_Code','money_ct1_Bin_Code', 'money_ct2_Bin_Code', 'money_ct3_Bin_Code',\\\n",
    "          'money_ct4_Bin_Code', 'money_ct5_Bin_Code', 'money_ct_Bin_Code', 'money_t1_Bin_Code',\\\n",
    "          'money_t2_Bin_Code', 'money_t3_Bin_Code', 'money_t4_Bin_Code', 'money_t5_Bin_Code',\\\n",
    "          'money_t_Bin_Code']\n",
    "\n",
    "colsta = ['current_score_ct', 'current_score_t','t_leads','round_status_BombPlanted',\\\n",
    "          'round_status_FreezeTime', 'round_status_Normal', 'round_status_time_left']\n",
    "\n",
    "colkill = ['kwct_Ak47', 'kwct_Aug', 'kwct_Awp', 'kwct_C4', 'kwct_Cz75Auto', 'kwct_Deagle',\\\n",
    "           'kwct_Flashbang', 'kwct_Glock', 'kwct_HeGrenade', 'kwct_Knife', 'kwct_M4a4',\\\n",
    "           'kwct_MolotovIncendiaryGrenade', 'kwct_Mp9', 'kwct_P2000', 'kwct_P250', 'kwct_Sg553',\\\n",
    "           'kwct_SmokeGrenade', 'kwct_UspS', 'kwct_other_heavy', 'kwct_other_pistols',\\\n",
    "           'kwct_other_rifles', 'kwct_other_smgs', 'kwct_other_utils', 'kwct_other_world', 'kwt_Ak47',\\\n",
    "           'kwt_Aug', 'kwt_Awp', 'kwt_C4', 'kwt_Cz75Auto', 'kwt_Deagle', 'kwt_Flashbang', 'kwt_Glock',\\\n",
    "           'kwt_HeGrenade', 'kwt_Knife', 'kwt_M4a4', 'kwt_MolotovIncendiaryGrenade', 'kwt_Mp9',\\\n",
    "           'kwt_P2000', 'kwt_P250', 'kwt_Sg553', 'kwt_SmokeGrenade', 'kwt_UspS', 'kwt_other_heavy',\\\n",
    "           'kwt_other_pistols', 'kwt_other_rifles', 'kwt_other_smgs', 'kwt_other_utils',\\\n",
    "           'kwt_other_world']\n",
    "\n",
    "colmap = ['map_de_dust2', 'map_de_inferno', 'map_de_mirage', 'map_de_nuke', 'map_de_overpass',\\\n",
    "          'map_de_train', 'map_de_vertigo','map_de_cache']\n",
    "\n",
    "colpos = ['pos_bs_ct1', 'pos_bs_ct2', 'pos_bs_ct3', 'pos_bs_ct4', 'pos_bs_ct5', 'pos_bs_t1',\\\n",
    "          'pos_bs_t2', 'pos_bs_t3', 'pos_bs_t4', 'pos_bs_t5']\n",
    "\n",
    "# colpos = ['pos_bs_ct1', 'pos_bs_ct2', 'pos_bs_ct3', 'pos_bs_ct4', 'pos_bs_ct5', 'pos_bs_t1',\\\n",
    "#           'pos_bs_t2', 'pos_bs_t3', 'pos_bs_t4', 'pos_bs_t5', 'pr_ct1','pr_ct2', 'pr_ct3',\\\n",
    "#           'pr_ct4', 'pr_ct5', 'pr_t1', 'pr_t2', 'pr_t3', 'pr_t4', 'pr_t5']\n",
    "\n",
    "cols = colpla+colmap+colwep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_all = df['round_winner_t']\n",
    "X_all = df.drop(columns='round_winner_t',axis=1)[cols]\n",
    "X_all = StandardScaler().fit_transform(X_all) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87141, 102)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87141,)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,X_t,Y_t,degree):\n",
    "    pr = PolynomialFeatures(degree=degree,include_bias=True)\n",
    "    X_p = pr.fit_transform(X_t)\n",
    "    model.fit(X_p,Y_t)\n",
    "    pred_train = model.predict(X_p)\n",
    "    \n",
    "    return pred_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model,X_t,degree):\n",
    "    pr = PolynomialFeatures(degree=degree,include_bias=True)\n",
    "    X_p = pr.fit_transform(X_t)\n",
    "    \n",
    "    return model.predict(X_p)\n",
    "\n",
    "def get_mse(model,Y_train,pred_train,Y_test,pred_test):\n",
    "    train_mse = mean_squared_error(Y_train, pred_train)\n",
    "    test_mse = mean_squared_error(Y_test, pred_test)\n",
    "    print(\"Training MSE = {}\".format(train_mse))\n",
    "    print(\"Test MSE = {}\".format(test_mse))\n",
    "\n",
    "def train_test(model,X_train,Y_train,X_test,Y_test,degree):\n",
    "    pred_train = train_model(model,X_train,Y_train,degree)\n",
    "    pred_test = test_model(model,X_test,degree)\n",
    "    print(pred_train)\n",
    "    get_mse(model,Y_train,pred_train,Y_test,pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_all, Y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLA = [\n",
    "    LinearRegression(),\n",
    "    LogisticRegression(solver='sag',verbose=1),\n",
    "    GaussianNB(),\n",
    "    MLPClassifier(verbose=1),\n",
    "    GradientBoostingClassifier(),\n",
    "#     VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)]),\n",
    "    BaggingClassifier(),\n",
    "    ExtraTreesClassifier(),\n",
    "    DecisionTreeClassifier(),\n",
    "    SVC(probability=True),\n",
    "    KNeighborsClassifier(n_neighbors = 3),\n",
    "    RandomForestClassifier(n_estimators = 100,verbose=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the classification on LinearRegression\n",
      "Running the classification on LogisticRegression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 59 epochs took 5 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    4.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 60 epochs took 5 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    4.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 61 epochs took 5 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    4.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 58 epochs took 5 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    4.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 59 epochs took 4 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    4.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 59 epochs took 4 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    4.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 61 epochs took 5 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    4.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 58 epochs took 5 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    4.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 57 epochs took 4 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    4.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 61 epochs took 5 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    4.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 45 epochs took 5 seconds\n",
      "Running the classification on GaussianNB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    4.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the classification on MLPClassifier\n",
      "Iteration 1, loss = 0.53850986\n",
      "Iteration 2, loss = 0.51088354\n",
      "Iteration 3, loss = 0.50121665\n",
      "Iteration 4, loss = 0.49380282\n",
      "Iteration 5, loss = 0.48728632\n",
      "Iteration 6, loss = 0.48008738\n",
      "Iteration 7, loss = 0.47502312\n",
      "Iteration 8, loss = 0.46921492\n",
      "Iteration 9, loss = 0.46355059\n",
      "Iteration 10, loss = 0.45855287\n",
      "Iteration 11, loss = 0.45343954\n",
      "Iteration 12, loss = 0.44972397\n",
      "Iteration 13, loss = 0.44593594\n",
      "Iteration 14, loss = 0.44091033\n",
      "Iteration 15, loss = 0.43769583\n",
      "Iteration 16, loss = 0.43433552\n",
      "Iteration 17, loss = 0.43120109\n",
      "Iteration 18, loss = 0.42705959\n",
      "Iteration 19, loss = 0.42385640\n",
      "Iteration 20, loss = 0.42107636\n",
      "Iteration 21, loss = 0.41757978\n",
      "Iteration 22, loss = 0.41500000\n",
      "Iteration 23, loss = 0.41300568\n",
      "Iteration 24, loss = 0.41003862\n",
      "Iteration 25, loss = 0.40753459\n",
      "Iteration 26, loss = 0.40572621\n",
      "Iteration 27, loss = 0.40361763\n",
      "Iteration 28, loss = 0.40292990\n",
      "Iteration 29, loss = 0.39938108\n",
      "Iteration 30, loss = 0.39717574\n",
      "Iteration 31, loss = 0.39519566\n",
      "Iteration 32, loss = 0.39316150\n",
      "Iteration 33, loss = 0.39285565\n",
      "Iteration 34, loss = 0.38883253\n",
      "Iteration 35, loss = 0.38799723\n",
      "Iteration 36, loss = 0.38629102\n",
      "Iteration 37, loss = 0.38485696\n",
      "Iteration 38, loss = 0.38436021\n",
      "Iteration 39, loss = 0.38174192\n",
      "Iteration 40, loss = 0.38015124\n",
      "Iteration 41, loss = 0.37881184\n",
      "Iteration 42, loss = 0.37734827\n",
      "Iteration 43, loss = 0.37666074\n",
      "Iteration 44, loss = 0.37568634\n",
      "Iteration 45, loss = 0.37334630\n",
      "Iteration 46, loss = 0.37290710\n",
      "Iteration 47, loss = 0.37085926\n",
      "Iteration 48, loss = 0.36972576\n",
      "Iteration 49, loss = 0.36784128\n",
      "Iteration 50, loss = 0.36668595\n",
      "Iteration 51, loss = 0.36600726\n",
      "Iteration 52, loss = 0.36561774\n",
      "Iteration 53, loss = 0.36388467\n",
      "Iteration 54, loss = 0.36168095\n",
      "Iteration 55, loss = 0.36114834\n",
      "Iteration 56, loss = 0.35985199\n",
      "Iteration 57, loss = 0.35943683\n",
      "Iteration 58, loss = 0.35794444\n",
      "Iteration 59, loss = 0.35691131\n",
      "Iteration 60, loss = 0.35561667\n",
      "Iteration 61, loss = 0.35589257\n",
      "Iteration 62, loss = 0.35317854\n",
      "Iteration 63, loss = 0.35451817\n",
      "Iteration 64, loss = 0.35128396\n",
      "Iteration 65, loss = 0.34956973\n",
      "Iteration 66, loss = 0.35013524\n",
      "Iteration 67, loss = 0.34927174\n",
      "Iteration 68, loss = 0.34813652\n",
      "Iteration 69, loss = 0.34704405\n",
      "Iteration 70, loss = 0.34671779\n",
      "Iteration 71, loss = 0.34636729\n",
      "Iteration 72, loss = 0.34430371\n",
      "Iteration 73, loss = 0.34541890\n",
      "Iteration 74, loss = 0.34354580\n",
      "Iteration 75, loss = 0.34220522\n",
      "Iteration 76, loss = 0.34236246\n",
      "Iteration 77, loss = 0.34088700\n",
      "Iteration 78, loss = 0.34059216\n",
      "Iteration 79, loss = 0.33969440\n",
      "Iteration 80, loss = 0.33897231\n",
      "Iteration 81, loss = 0.33810001\n",
      "Iteration 82, loss = 0.33662260\n",
      "Iteration 83, loss = 0.33663670\n",
      "Iteration 84, loss = 0.33600691\n",
      "Iteration 85, loss = 0.33533781\n",
      "Iteration 86, loss = 0.33568654\n",
      "Iteration 87, loss = 0.33395144\n",
      "Iteration 88, loss = 0.33572195\n",
      "Iteration 89, loss = 0.33222097\n",
      "Iteration 90, loss = 0.33248473\n",
      "Iteration 91, loss = 0.33142208\n",
      "Iteration 92, loss = 0.33153679\n",
      "Iteration 93, loss = 0.33202550\n",
      "Iteration 94, loss = 0.32944287\n",
      "Iteration 95, loss = 0.32769100\n",
      "Iteration 96, loss = 0.32798306\n",
      "Iteration 97, loss = 0.32784861\n",
      "Iteration 98, loss = 0.32763177\n",
      "Iteration 99, loss = 0.32599460\n",
      "Iteration 100, loss = 0.32522468\n",
      "Iteration 101, loss = 0.32549319\n",
      "Iteration 102, loss = 0.32578328\n",
      "Iteration 103, loss = 0.32463883\n",
      "Iteration 104, loss = 0.32419676\n",
      "Iteration 105, loss = 0.32250091\n",
      "Iteration 106, loss = 0.32398777\n",
      "Iteration 107, loss = 0.32218099\n",
      "Iteration 108, loss = 0.32299161\n",
      "Iteration 109, loss = 0.32214138\n",
      "Iteration 110, loss = 0.32286490\n",
      "Iteration 111, loss = 0.32080275\n",
      "Iteration 112, loss = 0.32176450\n",
      "Iteration 113, loss = 0.31914613\n",
      "Iteration 114, loss = 0.32031798\n",
      "Iteration 115, loss = 0.31888072\n",
      "Iteration 116, loss = 0.31811716\n",
      "Iteration 117, loss = 0.31950161\n",
      "Iteration 118, loss = 0.31728402\n",
      "Iteration 119, loss = 0.31725528\n",
      "Iteration 120, loss = 0.31718014\n",
      "Iteration 121, loss = 0.31600459\n",
      "Iteration 122, loss = 0.31562390\n",
      "Iteration 123, loss = 0.31681293\n",
      "Iteration 124, loss = 0.31601574\n",
      "Iteration 125, loss = 0.31347187\n",
      "Iteration 126, loss = 0.31312175\n",
      "Iteration 127, loss = 0.31543141\n",
      "Iteration 128, loss = 0.31402765\n",
      "Iteration 129, loss = 0.31261667\n",
      "Iteration 130, loss = 0.31446513\n",
      "Iteration 131, loss = 0.31267777\n",
      "Iteration 132, loss = 0.31262221\n",
      "Iteration 133, loss = 0.31219635\n",
      "Iteration 134, loss = 0.31148575\n",
      "Iteration 135, loss = 0.31203017\n",
      "Iteration 136, loss = 0.30988476\n",
      "Iteration 137, loss = 0.31182374\n",
      "Iteration 138, loss = 0.31059430\n",
      "Iteration 139, loss = 0.30922588\n",
      "Iteration 140, loss = 0.30976911\n",
      "Iteration 141, loss = 0.30862101\n",
      "Iteration 142, loss = 0.30906953\n",
      "Iteration 143, loss = 0.30928763\n",
      "Iteration 144, loss = 0.30841394\n",
      "Iteration 145, loss = 0.30773076\n",
      "Iteration 146, loss = 0.30689422\n",
      "Iteration 147, loss = 0.30698297\n",
      "Iteration 148, loss = 0.30846267\n",
      "Iteration 149, loss = 0.30750237\n",
      "Iteration 150, loss = 0.30490535\n",
      "Iteration 151, loss = 0.30642849\n",
      "Iteration 152, loss = 0.30631079\n",
      "Iteration 153, loss = 0.30377763\n",
      "Iteration 154, loss = 0.30456834\n",
      "Iteration 155, loss = 0.30486112\n",
      "Iteration 156, loss = 0.30504972\n",
      "Iteration 157, loss = 0.30539880\n",
      "Iteration 158, loss = 0.30244032\n",
      "Iteration 159, loss = 0.30368464\n",
      "Iteration 160, loss = 0.30289632\n",
      "Iteration 161, loss = 0.30311912\n",
      "Iteration 162, loss = 0.30185831\n",
      "Iteration 163, loss = 0.30126554\n",
      "Iteration 164, loss = 0.30291179\n",
      "Iteration 165, loss = 0.30262481\n",
      "Iteration 166, loss = 0.30152004\n",
      "Iteration 167, loss = 0.30086238\n",
      "Iteration 168, loss = 0.30141488\n",
      "Iteration 169, loss = 0.30028816\n",
      "Iteration 170, loss = 0.30090351\n",
      "Iteration 171, loss = 0.30085079\n",
      "Iteration 172, loss = 0.29919807\n",
      "Iteration 173, loss = 0.29942953\n",
      "Iteration 174, loss = 0.30009691\n",
      "Iteration 175, loss = 0.29903477\n",
      "Iteration 176, loss = 0.29806076\n",
      "Iteration 177, loss = 0.29867273\n",
      "Iteration 178, loss = 0.29924770\n",
      "Iteration 179, loss = 0.29631241\n",
      "Iteration 180, loss = 0.29847084\n",
      "Iteration 181, loss = 0.29902661\n",
      "Iteration 182, loss = 0.29633628\n",
      "Iteration 183, loss = 0.29877340\n",
      "Iteration 184, loss = 0.29735290\n",
      "Iteration 185, loss = 0.29698156\n",
      "Iteration 186, loss = 0.29705334\n",
      "Iteration 187, loss = 0.29696463\n",
      "Iteration 188, loss = 0.29488721\n",
      "Iteration 189, loss = 0.29555727\n",
      "Iteration 190, loss = 0.29685412\n",
      "Iteration 191, loss = 0.29465315\n",
      "Iteration 192, loss = 0.29488729\n",
      "Iteration 193, loss = 0.29434862\n",
      "Iteration 194, loss = 0.29398256\n",
      "Iteration 195, loss = 0.29440782\n",
      "Iteration 196, loss = 0.29624431\n",
      "Iteration 197, loss = 0.29535206\n",
      "Iteration 198, loss = 0.29445712\n",
      "Iteration 199, loss = 0.29228721\n",
      "Iteration 200, loss = 0.29313017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.53981026\n",
      "Iteration 2, loss = 0.51219928\n",
      "Iteration 3, loss = 0.50243588\n",
      "Iteration 4, loss = 0.49544134\n",
      "Iteration 5, loss = 0.48902826\n",
      "Iteration 6, loss = 0.48246627\n",
      "Iteration 7, loss = 0.47689044\n",
      "Iteration 8, loss = 0.47100763\n",
      "Iteration 9, loss = 0.46492795\n",
      "Iteration 10, loss = 0.46081811\n",
      "Iteration 11, loss = 0.45637675\n",
      "Iteration 12, loss = 0.45223921\n",
      "Iteration 13, loss = 0.44785779\n",
      "Iteration 14, loss = 0.44333222\n",
      "Iteration 15, loss = 0.43917051\n",
      "Iteration 16, loss = 0.43599513\n",
      "Iteration 17, loss = 0.43333602\n",
      "Iteration 18, loss = 0.43000378\n",
      "Iteration 19, loss = 0.42558830\n",
      "Iteration 20, loss = 0.42405458\n",
      "Iteration 21, loss = 0.42010750\n",
      "Iteration 22, loss = 0.41774353\n",
      "Iteration 23, loss = 0.41486376\n",
      "Iteration 24, loss = 0.41344213\n",
      "Iteration 25, loss = 0.40962965\n",
      "Iteration 26, loss = 0.40795035\n",
      "Iteration 27, loss = 0.40495995\n",
      "Iteration 28, loss = 0.40260740\n",
      "Iteration 29, loss = 0.40072907\n",
      "Iteration 30, loss = 0.39873291\n",
      "Iteration 31, loss = 0.39609429\n",
      "Iteration 32, loss = 0.39479579\n",
      "Iteration 33, loss = 0.39237727\n",
      "Iteration 34, loss = 0.38993659\n",
      "Iteration 35, loss = 0.38881734\n",
      "Iteration 36, loss = 0.38779528\n",
      "Iteration 37, loss = 0.38612031\n",
      "Iteration 38, loss = 0.38427156\n",
      "Iteration 39, loss = 0.38322661\n",
      "Iteration 40, loss = 0.38194497\n",
      "Iteration 41, loss = 0.37948503\n",
      "Iteration 42, loss = 0.37870048\n",
      "Iteration 43, loss = 0.37619073\n",
      "Iteration 44, loss = 0.37520233\n",
      "Iteration 45, loss = 0.37393109\n",
      "Iteration 46, loss = 0.37296647\n",
      "Iteration 47, loss = 0.37070601\n",
      "Iteration 48, loss = 0.36970161\n",
      "Iteration 49, loss = 0.36930772\n",
      "Iteration 50, loss = 0.36636068\n",
      "Iteration 51, loss = 0.36579980\n",
      "Iteration 52, loss = 0.36452153\n",
      "Iteration 53, loss = 0.36538161\n",
      "Iteration 54, loss = 0.36170462\n",
      "Iteration 55, loss = 0.36140634\n",
      "Iteration 56, loss = 0.36161562\n",
      "Iteration 57, loss = 0.36025562\n",
      "Iteration 58, loss = 0.35792659\n",
      "Iteration 59, loss = 0.35656752\n",
      "Iteration 60, loss = 0.35625008\n",
      "Iteration 61, loss = 0.35549762\n",
      "Iteration 62, loss = 0.35412428\n",
      "Iteration 63, loss = 0.35305697\n",
      "Iteration 64, loss = 0.35148046\n",
      "Iteration 65, loss = 0.35159007\n",
      "Iteration 66, loss = 0.35103392\n",
      "Iteration 67, loss = 0.35251269\n",
      "Iteration 68, loss = 0.34902469\n",
      "Iteration 69, loss = 0.34698610\n",
      "Iteration 70, loss = 0.34801743\n",
      "Iteration 71, loss = 0.34485977\n",
      "Iteration 72, loss = 0.34496555\n",
      "Iteration 73, loss = 0.34386957\n",
      "Iteration 74, loss = 0.34309095\n",
      "Iteration 75, loss = 0.34270142\n",
      "Iteration 76, loss = 0.34153325\n",
      "Iteration 77, loss = 0.34100569\n",
      "Iteration 78, loss = 0.34015225\n",
      "Iteration 79, loss = 0.33911018\n",
      "Iteration 80, loss = 0.34029316\n",
      "Iteration 81, loss = 0.33921871\n",
      "Iteration 82, loss = 0.33819356\n",
      "Iteration 83, loss = 0.33691080\n",
      "Iteration 84, loss = 0.33680696\n",
      "Iteration 85, loss = 0.33647806\n",
      "Iteration 86, loss = 0.33406779\n",
      "Iteration 87, loss = 0.33469702\n",
      "Iteration 88, loss = 0.33378173\n",
      "Iteration 89, loss = 0.33317410\n",
      "Iteration 90, loss = 0.33236792\n",
      "Iteration 91, loss = 0.33174162\n",
      "Iteration 92, loss = 0.33101685\n",
      "Iteration 93, loss = 0.33069391\n",
      "Iteration 94, loss = 0.33140778\n",
      "Iteration 95, loss = 0.33044831\n",
      "Iteration 96, loss = 0.33058224\n",
      "Iteration 97, loss = 0.32966240\n",
      "Iteration 98, loss = 0.32699946\n",
      "Iteration 99, loss = 0.32780647\n",
      "Iteration 100, loss = 0.32732447\n",
      "Iteration 101, loss = 0.32601134\n",
      "Iteration 102, loss = 0.32611622\n",
      "Iteration 103, loss = 0.32647594\n",
      "Iteration 104, loss = 0.32500734\n",
      "Iteration 105, loss = 0.32491633\n",
      "Iteration 106, loss = 0.32486950\n",
      "Iteration 107, loss = 0.32397157\n",
      "Iteration 108, loss = 0.32279581\n",
      "Iteration 109, loss = 0.32250013\n",
      "Iteration 110, loss = 0.32137419\n",
      "Iteration 111, loss = 0.32166182\n",
      "Iteration 112, loss = 0.32218975\n",
      "Iteration 113, loss = 0.32172355\n",
      "Iteration 114, loss = 0.32146811\n",
      "Iteration 115, loss = 0.32084053\n",
      "Iteration 116, loss = 0.31903414\n",
      "Iteration 117, loss = 0.31817263\n",
      "Iteration 118, loss = 0.31918717\n",
      "Iteration 119, loss = 0.32017720\n",
      "Iteration 120, loss = 0.31749845\n",
      "Iteration 121, loss = 0.31651296\n",
      "Iteration 122, loss = 0.31687953\n",
      "Iteration 123, loss = 0.31657966\n",
      "Iteration 124, loss = 0.31765031\n",
      "Iteration 125, loss = 0.31577725\n",
      "Iteration 126, loss = 0.31607813\n",
      "Iteration 127, loss = 0.31430813\n",
      "Iteration 128, loss = 0.31581684\n",
      "Iteration 129, loss = 0.31418316\n",
      "Iteration 130, loss = 0.31258416\n",
      "Iteration 131, loss = 0.31324689\n",
      "Iteration 132, loss = 0.31362576\n",
      "Iteration 133, loss = 0.31274085\n",
      "Iteration 134, loss = 0.31138940\n",
      "Iteration 135, loss = 0.31255962\n",
      "Iteration 136, loss = 0.31028795\n",
      "Iteration 137, loss = 0.31088961\n",
      "Iteration 138, loss = 0.31124822\n",
      "Iteration 139, loss = 0.31003167\n",
      "Iteration 140, loss = 0.30987820\n",
      "Iteration 141, loss = 0.31124332\n",
      "Iteration 142, loss = 0.30880022\n",
      "Iteration 143, loss = 0.30954258\n",
      "Iteration 144, loss = 0.30829142\n",
      "Iteration 145, loss = 0.30871361\n",
      "Iteration 146, loss = 0.30779066\n",
      "Iteration 147, loss = 0.30850786\n",
      "Iteration 148, loss = 0.30584922\n",
      "Iteration 149, loss = 0.30673612\n",
      "Iteration 150, loss = 0.30836885\n",
      "Iteration 151, loss = 0.30584729\n",
      "Iteration 152, loss = 0.30782767\n",
      "Iteration 153, loss = 0.30580968\n",
      "Iteration 154, loss = 0.30487210\n",
      "Iteration 155, loss = 0.30589119\n",
      "Iteration 156, loss = 0.30570170\n",
      "Iteration 157, loss = 0.30427401\n",
      "Iteration 158, loss = 0.30278101\n",
      "Iteration 159, loss = 0.30336190\n",
      "Iteration 160, loss = 0.30357791\n",
      "Iteration 161, loss = 0.30328180\n",
      "Iteration 162, loss = 0.30381441\n",
      "Iteration 163, loss = 0.30263920\n",
      "Iteration 164, loss = 0.30168653\n",
      "Iteration 165, loss = 0.30186375\n",
      "Iteration 166, loss = 0.30198158\n",
      "Iteration 167, loss = 0.30328113\n",
      "Iteration 168, loss = 0.30333313\n",
      "Iteration 169, loss = 0.30063840\n",
      "Iteration 170, loss = 0.30079449\n",
      "Iteration 171, loss = 0.29904595\n",
      "Iteration 172, loss = 0.30053647\n",
      "Iteration 173, loss = 0.29966466\n",
      "Iteration 174, loss = 0.30110239\n",
      "Iteration 175, loss = 0.29981424\n",
      "Iteration 176, loss = 0.29902458\n",
      "Iteration 177, loss = 0.30040289\n",
      "Iteration 178, loss = 0.29893796\n",
      "Iteration 179, loss = 0.29893610\n",
      "Iteration 180, loss = 0.29904245\n",
      "Iteration 181, loss = 0.29817002\n",
      "Iteration 182, loss = 0.29824182\n",
      "Iteration 183, loss = 0.29670592\n",
      "Iteration 184, loss = 0.29715051\n",
      "Iteration 185, loss = 0.29680767\n",
      "Iteration 186, loss = 0.29885350\n",
      "Iteration 187, loss = 0.29689835\n",
      "Iteration 188, loss = 0.29709403\n",
      "Iteration 189, loss = 0.29528900\n",
      "Iteration 190, loss = 0.29678041\n",
      "Iteration 191, loss = 0.29555490\n",
      "Iteration 192, loss = 0.29462944\n",
      "Iteration 193, loss = 0.29542575\n",
      "Iteration 194, loss = 0.29475107\n",
      "Iteration 195, loss = 0.29510014\n",
      "Iteration 196, loss = 0.29444959\n",
      "Iteration 197, loss = 0.29541298\n",
      "Iteration 198, loss = 0.29457168\n",
      "Iteration 199, loss = 0.29394594\n",
      "Iteration 200, loss = 0.29369091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.54039127\n",
      "Iteration 2, loss = 0.51261863\n",
      "Iteration 3, loss = 0.50345453\n",
      "Iteration 4, loss = 0.49612507\n",
      "Iteration 5, loss = 0.48933397\n",
      "Iteration 6, loss = 0.48261855\n",
      "Iteration 7, loss = 0.47696522\n",
      "Iteration 8, loss = 0.47109543\n",
      "Iteration 9, loss = 0.46565146\n",
      "Iteration 10, loss = 0.46093888\n",
      "Iteration 11, loss = 0.45677336\n",
      "Iteration 12, loss = 0.45114745\n",
      "Iteration 13, loss = 0.44690092\n",
      "Iteration 14, loss = 0.44374968\n",
      "Iteration 15, loss = 0.43916921\n",
      "Iteration 16, loss = 0.43546062\n",
      "Iteration 17, loss = 0.43244201\n",
      "Iteration 18, loss = 0.42948194\n",
      "Iteration 19, loss = 0.42697933\n",
      "Iteration 20, loss = 0.42326075\n",
      "Iteration 21, loss = 0.42092447\n",
      "Iteration 22, loss = 0.41873867\n",
      "Iteration 23, loss = 0.41520509\n",
      "Iteration 24, loss = 0.41239198\n",
      "Iteration 25, loss = 0.41073887\n",
      "Iteration 26, loss = 0.40927059\n",
      "Iteration 27, loss = 0.40570961\n",
      "Iteration 28, loss = 0.40353446\n",
      "Iteration 29, loss = 0.40147732\n",
      "Iteration 30, loss = 0.39920114\n",
      "Iteration 31, loss = 0.39754472\n",
      "Iteration 32, loss = 0.39529290\n",
      "Iteration 33, loss = 0.39288607\n",
      "Iteration 34, loss = 0.39105504\n",
      "Iteration 35, loss = 0.38913074\n",
      "Iteration 36, loss = 0.38699580\n",
      "Iteration 37, loss = 0.38625819\n",
      "Iteration 38, loss = 0.38483525\n",
      "Iteration 39, loss = 0.38147595\n",
      "Iteration 40, loss = 0.38165548\n",
      "Iteration 41, loss = 0.37931491\n",
      "Iteration 42, loss = 0.37772169\n",
      "Iteration 43, loss = 0.37573158\n",
      "Iteration 44, loss = 0.37403785\n",
      "Iteration 45, loss = 0.37289488\n",
      "Iteration 46, loss = 0.37072958\n",
      "Iteration 47, loss = 0.36962467\n",
      "Iteration 48, loss = 0.36863760\n",
      "Iteration 49, loss = 0.36802834\n",
      "Iteration 50, loss = 0.36715353\n",
      "Iteration 51, loss = 0.36554854\n",
      "Iteration 52, loss = 0.36393456\n",
      "Iteration 53, loss = 0.36255986\n",
      "Iteration 54, loss = 0.36286520\n",
      "Iteration 55, loss = 0.36321416\n",
      "Iteration 56, loss = 0.35933855\n",
      "Iteration 57, loss = 0.35966948\n",
      "Iteration 58, loss = 0.36008976\n",
      "Iteration 59, loss = 0.35771854\n",
      "Iteration 60, loss = 0.35489698\n",
      "Iteration 61, loss = 0.35526767\n",
      "Iteration 62, loss = 0.35376146\n",
      "Iteration 63, loss = 0.35258693\n",
      "Iteration 64, loss = 0.35200618\n",
      "Iteration 65, loss = 0.35043860\n",
      "Iteration 66, loss = 0.35088837\n",
      "Iteration 67, loss = 0.34979779\n",
      "Iteration 68, loss = 0.34778912\n",
      "Iteration 69, loss = 0.34908381\n",
      "Iteration 70, loss = 0.34592517\n",
      "Iteration 71, loss = 0.34583770\n",
      "Iteration 72, loss = 0.34548484\n",
      "Iteration 73, loss = 0.34510710\n",
      "Iteration 74, loss = 0.34385767\n",
      "Iteration 75, loss = 0.34428243\n",
      "Iteration 76, loss = 0.34274129\n",
      "Iteration 77, loss = 0.34255914\n",
      "Iteration 78, loss = 0.34185024\n",
      "Iteration 79, loss = 0.34040627\n",
      "Iteration 80, loss = 0.34154041\n",
      "Iteration 81, loss = 0.34060810\n",
      "Iteration 82, loss = 0.33856462\n",
      "Iteration 83, loss = 0.33813918\n",
      "Iteration 84, loss = 0.33717688\n",
      "Iteration 85, loss = 0.33646799\n",
      "Iteration 86, loss = 0.33603249\n",
      "Iteration 87, loss = 0.33390125\n",
      "Iteration 88, loss = 0.33313444\n",
      "Iteration 89, loss = 0.33322180\n",
      "Iteration 90, loss = 0.33326794\n",
      "Iteration 91, loss = 0.33267201\n",
      "Iteration 92, loss = 0.33214399\n",
      "Iteration 93, loss = 0.33195244\n",
      "Iteration 94, loss = 0.32996304\n",
      "Iteration 95, loss = 0.33021395\n",
      "Iteration 96, loss = 0.33010691\n",
      "Iteration 97, loss = 0.32952103\n",
      "Iteration 98, loss = 0.33123635\n",
      "Iteration 99, loss = 0.32813140\n",
      "Iteration 100, loss = 0.32605463\n",
      "Iteration 101, loss = 0.32633758\n",
      "Iteration 102, loss = 0.32662227\n",
      "Iteration 103, loss = 0.32598363\n",
      "Iteration 104, loss = 0.32574518\n",
      "Iteration 105, loss = 0.32667425\n",
      "Iteration 106, loss = 0.32551249\n",
      "Iteration 107, loss = 0.32232519\n",
      "Iteration 108, loss = 0.32243659\n",
      "Iteration 109, loss = 0.32200991\n",
      "Iteration 110, loss = 0.32364528\n",
      "Iteration 111, loss = 0.32163361\n",
      "Iteration 112, loss = 0.32098163\n",
      "Iteration 113, loss = 0.32068991\n",
      "Iteration 114, loss = 0.32139551\n",
      "Iteration 115, loss = 0.32009759\n",
      "Iteration 116, loss = 0.31978526\n",
      "Iteration 117, loss = 0.31911014\n",
      "Iteration 118, loss = 0.31761401\n",
      "Iteration 119, loss = 0.31774305\n",
      "Iteration 120, loss = 0.31827497\n",
      "Iteration 121, loss = 0.31911630\n",
      "Iteration 122, loss = 0.31752216\n",
      "Iteration 123, loss = 0.31603923\n",
      "Iteration 124, loss = 0.31690388\n",
      "Iteration 125, loss = 0.31519854\n",
      "Iteration 126, loss = 0.31417497\n",
      "Iteration 127, loss = 0.31400265\n",
      "Iteration 128, loss = 0.31464703\n",
      "Iteration 129, loss = 0.31416337\n",
      "Iteration 130, loss = 0.31471669\n",
      "Iteration 131, loss = 0.31402963\n",
      "Iteration 132, loss = 0.31223113\n",
      "Iteration 133, loss = 0.31349746\n",
      "Iteration 134, loss = 0.31327465\n",
      "Iteration 135, loss = 0.31181913\n",
      "Iteration 136, loss = 0.31334783\n",
      "Iteration 137, loss = 0.31140901\n",
      "Iteration 138, loss = 0.30977850\n",
      "Iteration 139, loss = 0.31000130\n",
      "Iteration 140, loss = 0.31001943\n",
      "Iteration 141, loss = 0.31097333\n",
      "Iteration 142, loss = 0.31075788\n",
      "Iteration 143, loss = 0.30891920\n",
      "Iteration 144, loss = 0.30889290\n",
      "Iteration 145, loss = 0.30957811\n",
      "Iteration 146, loss = 0.30806389\n",
      "Iteration 147, loss = 0.30844289\n",
      "Iteration 148, loss = 0.30628299\n",
      "Iteration 149, loss = 0.30723177\n",
      "Iteration 150, loss = 0.30703230\n",
      "Iteration 151, loss = 0.30666874\n",
      "Iteration 152, loss = 0.30806699\n",
      "Iteration 153, loss = 0.30686367\n",
      "Iteration 154, loss = 0.30535295\n",
      "Iteration 155, loss = 0.30549715\n",
      "Iteration 156, loss = 0.30513686\n",
      "Iteration 157, loss = 0.30454033\n",
      "Iteration 158, loss = 0.30509894\n",
      "Iteration 159, loss = 0.30488927\n",
      "Iteration 160, loss = 0.30331732\n",
      "Iteration 161, loss = 0.30438607\n",
      "Iteration 162, loss = 0.30368904\n",
      "Iteration 163, loss = 0.30490657\n",
      "Iteration 164, loss = 0.30297056\n",
      "Iteration 165, loss = 0.30460227\n",
      "Iteration 166, loss = 0.30317076\n",
      "Iteration 167, loss = 0.30133400\n",
      "Iteration 168, loss = 0.30131885\n",
      "Iteration 169, loss = 0.30098355\n",
      "Iteration 170, loss = 0.30202227\n",
      "Iteration 171, loss = 0.30266815\n",
      "Iteration 172, loss = 0.30146860\n",
      "Iteration 173, loss = 0.30145099\n",
      "Iteration 174, loss = 0.30035205\n",
      "Iteration 175, loss = 0.30016432\n",
      "Iteration 176, loss = 0.30048780\n",
      "Iteration 177, loss = 0.30096610\n",
      "Iteration 178, loss = 0.29766776\n",
      "Iteration 179, loss = 0.29948871\n",
      "Iteration 180, loss = 0.29902136\n",
      "Iteration 181, loss = 0.29818004\n",
      "Iteration 182, loss = 0.29877999\n",
      "Iteration 183, loss = 0.29789917\n",
      "Iteration 184, loss = 0.29797068\n",
      "Iteration 185, loss = 0.29837527\n",
      "Iteration 186, loss = 0.29785435\n",
      "Iteration 187, loss = 0.29660100\n",
      "Iteration 188, loss = 0.29738764\n",
      "Iteration 189, loss = 0.29792440\n",
      "Iteration 190, loss = 0.29718124\n",
      "Iteration 191, loss = 0.29701219\n",
      "Iteration 192, loss = 0.29705134\n",
      "Iteration 193, loss = 0.29766747\n",
      "Iteration 194, loss = 0.29575334\n",
      "Iteration 195, loss = 0.29627279\n",
      "Iteration 196, loss = 0.29724762\n",
      "Iteration 197, loss = 0.29593642\n",
      "Iteration 198, loss = 0.29494100\n",
      "Iteration 199, loss = 0.29447668\n",
      "Iteration 200, loss = 0.29462119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.53763180\n",
      "Iteration 2, loss = 0.51047527\n",
      "Iteration 3, loss = 0.50117827\n",
      "Iteration 4, loss = 0.49381403\n",
      "Iteration 5, loss = 0.48672163\n",
      "Iteration 6, loss = 0.48069374\n",
      "Iteration 7, loss = 0.47405357\n",
      "Iteration 8, loss = 0.46871805\n",
      "Iteration 9, loss = 0.46411598\n",
      "Iteration 10, loss = 0.45929242\n",
      "Iteration 11, loss = 0.45404788\n",
      "Iteration 12, loss = 0.44921509\n",
      "Iteration 13, loss = 0.44508455\n",
      "Iteration 14, loss = 0.44203658\n",
      "Iteration 15, loss = 0.43725782\n",
      "Iteration 16, loss = 0.43361163\n",
      "Iteration 17, loss = 0.43043126\n",
      "Iteration 18, loss = 0.42622928\n",
      "Iteration 19, loss = 0.42365145\n",
      "Iteration 20, loss = 0.42126327\n",
      "Iteration 21, loss = 0.41737621\n",
      "Iteration 22, loss = 0.41548258\n",
      "Iteration 23, loss = 0.41376044\n",
      "Iteration 24, loss = 0.41064959\n",
      "Iteration 25, loss = 0.40828509\n",
      "Iteration 26, loss = 0.40538512\n",
      "Iteration 27, loss = 0.40276352\n",
      "Iteration 28, loss = 0.40063339\n",
      "Iteration 29, loss = 0.39795069\n",
      "Iteration 30, loss = 0.39565815\n",
      "Iteration 31, loss = 0.39391185\n",
      "Iteration 32, loss = 0.39194604\n",
      "Iteration 33, loss = 0.39109597\n",
      "Iteration 34, loss = 0.38871700\n",
      "Iteration 35, loss = 0.38694981\n",
      "Iteration 36, loss = 0.38388782\n",
      "Iteration 37, loss = 0.38242420\n",
      "Iteration 38, loss = 0.38130997\n",
      "Iteration 39, loss = 0.38032486\n",
      "Iteration 40, loss = 0.37797905\n",
      "Iteration 41, loss = 0.37649256\n",
      "Iteration 42, loss = 0.37508799\n",
      "Iteration 43, loss = 0.37539041\n",
      "Iteration 44, loss = 0.37188782\n",
      "Iteration 45, loss = 0.37074842\n",
      "Iteration 46, loss = 0.37011310\n",
      "Iteration 47, loss = 0.36922073\n",
      "Iteration 48, loss = 0.36715629\n",
      "Iteration 49, loss = 0.36644988\n",
      "Iteration 50, loss = 0.36524174\n",
      "Iteration 51, loss = 0.36428185\n",
      "Iteration 52, loss = 0.36216971\n",
      "Iteration 53, loss = 0.36118065\n",
      "Iteration 54, loss = 0.35957848\n",
      "Iteration 55, loss = 0.35864812\n",
      "Iteration 56, loss = 0.35847350\n",
      "Iteration 57, loss = 0.35824814\n",
      "Iteration 58, loss = 0.35669212\n",
      "Iteration 59, loss = 0.35572206\n",
      "Iteration 60, loss = 0.35411715\n",
      "Iteration 61, loss = 0.35247639\n",
      "Iteration 62, loss = 0.35145511\n",
      "Iteration 63, loss = 0.35131753\n",
      "Iteration 64, loss = 0.35035637\n",
      "Iteration 65, loss = 0.34938424\n",
      "Iteration 66, loss = 0.34940216\n",
      "Iteration 67, loss = 0.34859598\n",
      "Iteration 68, loss = 0.34706764\n",
      "Iteration 69, loss = 0.34534104\n",
      "Iteration 70, loss = 0.34516397\n",
      "Iteration 71, loss = 0.34432047\n",
      "Iteration 72, loss = 0.34479458\n",
      "Iteration 73, loss = 0.34271612\n",
      "Iteration 74, loss = 0.34284572\n",
      "Iteration 75, loss = 0.34214041\n",
      "Iteration 76, loss = 0.34196489\n",
      "Iteration 77, loss = 0.33980269\n",
      "Iteration 78, loss = 0.34010778\n",
      "Iteration 79, loss = 0.33976136\n",
      "Iteration 80, loss = 0.33880950\n",
      "Iteration 81, loss = 0.33811534\n",
      "Iteration 82, loss = 0.33660167\n",
      "Iteration 83, loss = 0.33746020\n",
      "Iteration 84, loss = 0.33508462\n",
      "Iteration 85, loss = 0.33613836\n",
      "Iteration 86, loss = 0.33407881\n",
      "Iteration 87, loss = 0.33327418\n",
      "Iteration 88, loss = 0.33301998\n",
      "Iteration 89, loss = 0.33275328\n",
      "Iteration 90, loss = 0.33215439\n",
      "Iteration 91, loss = 0.33239111\n",
      "Iteration 92, loss = 0.32996919\n",
      "Iteration 93, loss = 0.33021361\n",
      "Iteration 94, loss = 0.33001126\n",
      "Iteration 95, loss = 0.32921982\n",
      "Iteration 96, loss = 0.32830040\n",
      "Iteration 97, loss = 0.32928177\n",
      "Iteration 98, loss = 0.32763633\n",
      "Iteration 99, loss = 0.32745952\n",
      "Iteration 100, loss = 0.32685381\n",
      "Iteration 101, loss = 0.32607437\n",
      "Iteration 102, loss = 0.32601815\n",
      "Iteration 103, loss = 0.32517893\n",
      "Iteration 104, loss = 0.32442132\n",
      "Iteration 105, loss = 0.32388082\n",
      "Iteration 106, loss = 0.32470219\n",
      "Iteration 107, loss = 0.32316501\n",
      "Iteration 108, loss = 0.32217591\n",
      "Iteration 109, loss = 0.32233530\n",
      "Iteration 110, loss = 0.32166267\n",
      "Iteration 111, loss = 0.32081327\n",
      "Iteration 112, loss = 0.31907682\n",
      "Iteration 113, loss = 0.31988392\n",
      "Iteration 114, loss = 0.32025654\n",
      "Iteration 115, loss = 0.31929403\n",
      "Iteration 116, loss = 0.31984606\n",
      "Iteration 117, loss = 0.31835330\n",
      "Iteration 118, loss = 0.31842263\n",
      "Iteration 119, loss = 0.31695144\n",
      "Iteration 120, loss = 0.31716584\n",
      "Iteration 121, loss = 0.31832054\n",
      "Iteration 122, loss = 0.31645301\n",
      "Iteration 123, loss = 0.31389346\n",
      "Iteration 124, loss = 0.31433091\n",
      "Iteration 125, loss = 0.31616629\n",
      "Iteration 126, loss = 0.31677053\n",
      "Iteration 127, loss = 0.31336451\n",
      "Iteration 128, loss = 0.31457216\n",
      "Iteration 129, loss = 0.31292549\n",
      "Iteration 130, loss = 0.31320724\n",
      "Iteration 131, loss = 0.31223469\n",
      "Iteration 132, loss = 0.31338432\n",
      "Iteration 133, loss = 0.31167295\n",
      "Iteration 134, loss = 0.31250534\n",
      "Iteration 135, loss = 0.30922545\n",
      "Iteration 136, loss = 0.31161321\n",
      "Iteration 137, loss = 0.30908121\n",
      "Iteration 138, loss = 0.31018091\n",
      "Iteration 139, loss = 0.30951365\n",
      "Iteration 140, loss = 0.30832688\n",
      "Iteration 141, loss = 0.30911667\n",
      "Iteration 142, loss = 0.30870311\n",
      "Iteration 143, loss = 0.30868701\n",
      "Iteration 144, loss = 0.30900466\n",
      "Iteration 145, loss = 0.30733065\n",
      "Iteration 146, loss = 0.30778675\n",
      "Iteration 147, loss = 0.30657367\n",
      "Iteration 148, loss = 0.30544919\n",
      "Iteration 149, loss = 0.30644959\n",
      "Iteration 150, loss = 0.30564001\n",
      "Iteration 151, loss = 0.30539287\n",
      "Iteration 152, loss = 0.30677203\n",
      "Iteration 153, loss = 0.30640213\n",
      "Iteration 154, loss = 0.30563283\n",
      "Iteration 155, loss = 0.30549385\n",
      "Iteration 156, loss = 0.30432314\n",
      "Iteration 157, loss = 0.30468835\n",
      "Iteration 158, loss = 0.30283136\n",
      "Iteration 159, loss = 0.30431994\n",
      "Iteration 160, loss = 0.30247342\n",
      "Iteration 161, loss = 0.30326497\n",
      "Iteration 162, loss = 0.30382884\n",
      "Iteration 163, loss = 0.30280330\n",
      "Iteration 164, loss = 0.30180779\n",
      "Iteration 165, loss = 0.30343316\n",
      "Iteration 166, loss = 0.30056589\n",
      "Iteration 167, loss = 0.30019516\n",
      "Iteration 168, loss = 0.30070707\n",
      "Iteration 169, loss = 0.29996185\n",
      "Iteration 170, loss = 0.30052433\n",
      "Iteration 171, loss = 0.30033442\n",
      "Iteration 172, loss = 0.30057618\n",
      "Iteration 173, loss = 0.29895077\n",
      "Iteration 174, loss = 0.29985955\n",
      "Iteration 175, loss = 0.30090113\n",
      "Iteration 176, loss = 0.29862670\n",
      "Iteration 177, loss = 0.29938149\n",
      "Iteration 178, loss = 0.29825866\n",
      "Iteration 179, loss = 0.29900025\n",
      "Iteration 180, loss = 0.29980828\n",
      "Iteration 181, loss = 0.29953206\n",
      "Iteration 182, loss = 0.29776296\n",
      "Iteration 183, loss = 0.29769588\n",
      "Iteration 184, loss = 0.29917069\n",
      "Iteration 185, loss = 0.29831823\n",
      "Iteration 186, loss = 0.29778525\n",
      "Iteration 187, loss = 0.29637250\n",
      "Iteration 188, loss = 0.29556625\n",
      "Iteration 189, loss = 0.29869762\n",
      "Iteration 190, loss = 0.29615205\n",
      "Iteration 191, loss = 0.29499371\n",
      "Iteration 192, loss = 0.29721995\n",
      "Iteration 193, loss = 0.29555022\n",
      "Iteration 194, loss = 0.29571073\n",
      "Iteration 195, loss = 0.29559265\n",
      "Iteration 196, loss = 0.29339703\n",
      "Iteration 197, loss = 0.29413080\n",
      "Iteration 198, loss = 0.29510381\n",
      "Iteration 199, loss = 0.29496893\n",
      "Iteration 200, loss = 0.29428212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.53887081\n",
      "Iteration 2, loss = 0.50985836\n",
      "Iteration 3, loss = 0.49984525\n",
      "Iteration 4, loss = 0.49286947\n",
      "Iteration 5, loss = 0.48595868\n",
      "Iteration 6, loss = 0.47976689\n",
      "Iteration 7, loss = 0.47398502\n",
      "Iteration 8, loss = 0.46803075\n",
      "Iteration 9, loss = 0.46353279\n",
      "Iteration 10, loss = 0.45870615\n",
      "Iteration 11, loss = 0.45376943\n",
      "Iteration 12, loss = 0.44961328\n",
      "Iteration 13, loss = 0.44538129\n",
      "Iteration 14, loss = 0.44156257\n",
      "Iteration 15, loss = 0.43770302\n",
      "Iteration 16, loss = 0.43335817\n",
      "Iteration 17, loss = 0.43036617\n",
      "Iteration 18, loss = 0.42680510\n",
      "Iteration 19, loss = 0.42466772\n",
      "Iteration 20, loss = 0.42089199\n",
      "Iteration 21, loss = 0.41861611\n",
      "Iteration 22, loss = 0.41431353\n",
      "Iteration 23, loss = 0.41270687\n",
      "Iteration 24, loss = 0.40934435\n",
      "Iteration 25, loss = 0.40851022\n",
      "Iteration 26, loss = 0.40604174\n",
      "Iteration 27, loss = 0.40435653\n",
      "Iteration 28, loss = 0.40084448\n",
      "Iteration 29, loss = 0.39795651\n",
      "Iteration 30, loss = 0.39663882\n",
      "Iteration 31, loss = 0.39376051\n",
      "Iteration 32, loss = 0.39262763\n",
      "Iteration 33, loss = 0.39040585\n",
      "Iteration 34, loss = 0.38903281\n",
      "Iteration 35, loss = 0.38634434\n",
      "Iteration 36, loss = 0.38488010\n",
      "Iteration 37, loss = 0.38385210\n",
      "Iteration 38, loss = 0.38127044\n",
      "Iteration 39, loss = 0.37998084\n",
      "Iteration 40, loss = 0.37853732\n",
      "Iteration 41, loss = 0.37684134\n",
      "Iteration 42, loss = 0.37481715\n",
      "Iteration 43, loss = 0.37311531\n",
      "Iteration 44, loss = 0.37190055\n",
      "Iteration 45, loss = 0.36981709\n",
      "Iteration 46, loss = 0.36905641\n",
      "Iteration 47, loss = 0.36884295\n",
      "Iteration 48, loss = 0.36640766\n",
      "Iteration 49, loss = 0.36575062\n",
      "Iteration 50, loss = 0.36465569\n",
      "Iteration 51, loss = 0.36300986\n",
      "Iteration 52, loss = 0.36125716\n",
      "Iteration 53, loss = 0.36051699\n",
      "Iteration 54, loss = 0.36040721\n",
      "Iteration 55, loss = 0.35681680\n",
      "Iteration 56, loss = 0.35690174\n",
      "Iteration 57, loss = 0.35611564\n",
      "Iteration 58, loss = 0.35523539\n",
      "Iteration 59, loss = 0.35387789\n",
      "Iteration 60, loss = 0.35394340\n",
      "Iteration 61, loss = 0.35219999\n",
      "Iteration 62, loss = 0.35202616\n",
      "Iteration 63, loss = 0.34922694\n",
      "Iteration 64, loss = 0.34983771\n",
      "Iteration 65, loss = 0.34836983\n",
      "Iteration 66, loss = 0.34678978\n",
      "Iteration 67, loss = 0.34617347\n",
      "Iteration 68, loss = 0.34389191\n",
      "Iteration 69, loss = 0.34505214\n",
      "Iteration 70, loss = 0.34294322\n",
      "Iteration 71, loss = 0.34255033\n",
      "Iteration 72, loss = 0.34194680\n",
      "Iteration 73, loss = 0.34188972\n",
      "Iteration 74, loss = 0.34104413\n",
      "Iteration 75, loss = 0.33968269\n",
      "Iteration 76, loss = 0.33953061\n",
      "Iteration 77, loss = 0.33692514\n",
      "Iteration 78, loss = 0.33601437\n",
      "Iteration 79, loss = 0.33586322\n",
      "Iteration 80, loss = 0.33498719\n",
      "Iteration 81, loss = 0.33456043\n",
      "Iteration 82, loss = 0.33566690\n",
      "Iteration 83, loss = 0.33368469\n",
      "Iteration 84, loss = 0.33376111\n",
      "Iteration 85, loss = 0.33179366\n",
      "Iteration 86, loss = 0.33107204\n",
      "Iteration 87, loss = 0.33165962\n",
      "Iteration 88, loss = 0.33109542\n",
      "Iteration 89, loss = 0.33056983\n",
      "Iteration 90, loss = 0.32843536\n",
      "Iteration 91, loss = 0.32775166\n",
      "Iteration 92, loss = 0.32673199\n",
      "Iteration 93, loss = 0.32756292\n",
      "Iteration 94, loss = 0.32440345\n",
      "Iteration 95, loss = 0.32508449\n",
      "Iteration 96, loss = 0.32471870\n",
      "Iteration 97, loss = 0.32490321\n",
      "Iteration 98, loss = 0.32545900\n",
      "Iteration 99, loss = 0.32183679\n",
      "Iteration 100, loss = 0.32233076\n",
      "Iteration 101, loss = 0.32358370\n",
      "Iteration 102, loss = 0.32159369\n",
      "Iteration 103, loss = 0.32091110\n",
      "Iteration 104, loss = 0.32100786\n",
      "Iteration 105, loss = 0.31936650\n",
      "Iteration 106, loss = 0.31911937\n",
      "Iteration 107, loss = 0.32024442\n",
      "Iteration 108, loss = 0.31781647\n",
      "Iteration 109, loss = 0.31857640\n",
      "Iteration 110, loss = 0.31716445\n",
      "Iteration 111, loss = 0.31694676\n",
      "Iteration 112, loss = 0.31734570\n",
      "Iteration 113, loss = 0.31732800\n",
      "Iteration 114, loss = 0.31485345\n",
      "Iteration 115, loss = 0.31605090\n",
      "Iteration 116, loss = 0.31449117\n",
      "Iteration 117, loss = 0.31479501\n",
      "Iteration 118, loss = 0.31413961\n",
      "Iteration 119, loss = 0.31538304\n",
      "Iteration 120, loss = 0.31306148\n",
      "Iteration 121, loss = 0.31335912\n",
      "Iteration 122, loss = 0.31274307\n",
      "Iteration 123, loss = 0.31087575\n",
      "Iteration 124, loss = 0.31231317\n",
      "Iteration 125, loss = 0.31019300\n",
      "Iteration 126, loss = 0.31127112\n",
      "Iteration 127, loss = 0.31064215\n",
      "Iteration 128, loss = 0.30983135\n",
      "Iteration 129, loss = 0.30958741\n",
      "Iteration 130, loss = 0.31021166\n",
      "Iteration 131, loss = 0.30968573\n",
      "Iteration 132, loss = 0.30846824\n",
      "Iteration 133, loss = 0.30843862\n",
      "Iteration 134, loss = 0.30807315\n",
      "Iteration 135, loss = 0.30787887\n",
      "Iteration 136, loss = 0.30763348\n",
      "Iteration 137, loss = 0.30649850\n",
      "Iteration 138, loss = 0.30691147\n",
      "Iteration 139, loss = 0.30563416\n",
      "Iteration 140, loss = 0.30574823\n",
      "Iteration 141, loss = 0.30637602\n",
      "Iteration 142, loss = 0.30711179\n",
      "Iteration 143, loss = 0.30480891\n",
      "Iteration 144, loss = 0.30430191\n",
      "Iteration 145, loss = 0.30521534\n",
      "Iteration 146, loss = 0.30328962\n",
      "Iteration 147, loss = 0.30246986\n",
      "Iteration 148, loss = 0.30306415\n",
      "Iteration 149, loss = 0.30442960\n",
      "Iteration 150, loss = 0.30222489\n",
      "Iteration 151, loss = 0.30170654\n",
      "Iteration 152, loss = 0.30166880\n",
      "Iteration 153, loss = 0.30155633\n",
      "Iteration 154, loss = 0.30102702\n",
      "Iteration 155, loss = 0.30184906\n",
      "Iteration 156, loss = 0.30160742\n",
      "Iteration 157, loss = 0.30010966\n",
      "Iteration 158, loss = 0.30129609\n",
      "Iteration 159, loss = 0.29853683\n",
      "Iteration 160, loss = 0.29931043\n",
      "Iteration 161, loss = 0.29889567\n",
      "Iteration 162, loss = 0.29970889\n",
      "Iteration 163, loss = 0.29915844\n",
      "Iteration 164, loss = 0.29770431\n",
      "Iteration 165, loss = 0.29800423\n",
      "Iteration 166, loss = 0.29882551\n",
      "Iteration 167, loss = 0.29731233\n",
      "Iteration 168, loss = 0.29675012\n",
      "Iteration 169, loss = 0.29691478\n",
      "Iteration 170, loss = 0.29728191\n",
      "Iteration 171, loss = 0.29629310\n",
      "Iteration 172, loss = 0.29488305\n",
      "Iteration 173, loss = 0.29533558\n",
      "Iteration 174, loss = 0.29535677\n",
      "Iteration 175, loss = 0.29415962\n",
      "Iteration 176, loss = 0.29622577\n",
      "Iteration 177, loss = 0.29478927\n",
      "Iteration 178, loss = 0.29507904\n",
      "Iteration 179, loss = 0.29292171\n",
      "Iteration 180, loss = 0.29430346\n",
      "Iteration 181, loss = 0.29345162\n",
      "Iteration 182, loss = 0.29193455\n",
      "Iteration 183, loss = 0.29305836\n",
      "Iteration 184, loss = 0.29425308\n",
      "Iteration 185, loss = 0.29179515\n",
      "Iteration 186, loss = 0.29452542\n",
      "Iteration 187, loss = 0.29317699\n",
      "Iteration 188, loss = 0.29091649\n",
      "Iteration 189, loss = 0.29053788\n",
      "Iteration 190, loss = 0.29084210\n",
      "Iteration 191, loss = 0.29169355\n",
      "Iteration 192, loss = 0.29150713\n",
      "Iteration 193, loss = 0.28912585\n",
      "Iteration 194, loss = 0.29011881\n",
      "Iteration 195, loss = 0.28898944\n",
      "Iteration 196, loss = 0.29136594\n",
      "Iteration 197, loss = 0.28990549\n",
      "Iteration 198, loss = 0.28841706\n",
      "Iteration 199, loss = 0.28759122\n",
      "Iteration 200, loss = 0.28780826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.53739183\n",
      "Iteration 2, loss = 0.51131873\n",
      "Iteration 3, loss = 0.50085188\n",
      "Iteration 4, loss = 0.49325848\n",
      "Iteration 5, loss = 0.48509223\n",
      "Iteration 6, loss = 0.47903576\n",
      "Iteration 7, loss = 0.47318119\n",
      "Iteration 8, loss = 0.46692729\n",
      "Iteration 9, loss = 0.46172412\n",
      "Iteration 10, loss = 0.45626013\n",
      "Iteration 11, loss = 0.45151457\n",
      "Iteration 12, loss = 0.44712063\n",
      "Iteration 13, loss = 0.44371465\n",
      "Iteration 14, loss = 0.44000435\n",
      "Iteration 15, loss = 0.43553960\n",
      "Iteration 16, loss = 0.43321613\n",
      "Iteration 17, loss = 0.42853862\n",
      "Iteration 18, loss = 0.42700787\n",
      "Iteration 19, loss = 0.42319544\n",
      "Iteration 20, loss = 0.42046479\n",
      "Iteration 21, loss = 0.41792781\n",
      "Iteration 22, loss = 0.41487426\n",
      "Iteration 23, loss = 0.41211918\n",
      "Iteration 24, loss = 0.41008083\n",
      "Iteration 25, loss = 0.40778505\n",
      "Iteration 26, loss = 0.40580822\n",
      "Iteration 27, loss = 0.40388019\n",
      "Iteration 28, loss = 0.40168469\n",
      "Iteration 29, loss = 0.39938238\n",
      "Iteration 30, loss = 0.39718315\n",
      "Iteration 31, loss = 0.39547707\n",
      "Iteration 32, loss = 0.39317008\n",
      "Iteration 33, loss = 0.39108231\n",
      "Iteration 34, loss = 0.38941823\n",
      "Iteration 35, loss = 0.38673953\n",
      "Iteration 36, loss = 0.38618815\n",
      "Iteration 37, loss = 0.38406618\n",
      "Iteration 38, loss = 0.38283979\n",
      "Iteration 39, loss = 0.38090386\n",
      "Iteration 40, loss = 0.37995977\n",
      "Iteration 41, loss = 0.37880403\n",
      "Iteration 42, loss = 0.37733243\n",
      "Iteration 43, loss = 0.37514924\n",
      "Iteration 44, loss = 0.37510060\n",
      "Iteration 45, loss = 0.37198741\n",
      "Iteration 46, loss = 0.37338774\n",
      "Iteration 47, loss = 0.37081130\n",
      "Iteration 48, loss = 0.36806732\n",
      "Iteration 49, loss = 0.36717251\n",
      "Iteration 50, loss = 0.36726819\n",
      "Iteration 51, loss = 0.36507987\n",
      "Iteration 52, loss = 0.36487575\n",
      "Iteration 53, loss = 0.36248051\n",
      "Iteration 54, loss = 0.36295657\n",
      "Iteration 55, loss = 0.36085040\n",
      "Iteration 56, loss = 0.36051142\n",
      "Iteration 57, loss = 0.35908453\n",
      "Iteration 58, loss = 0.35796442\n",
      "Iteration 59, loss = 0.35759589\n",
      "Iteration 60, loss = 0.35688094\n",
      "Iteration 61, loss = 0.35465625\n",
      "Iteration 62, loss = 0.35441409\n",
      "Iteration 63, loss = 0.35228709\n",
      "Iteration 64, loss = 0.35117948\n",
      "Iteration 65, loss = 0.35256521\n",
      "Iteration 66, loss = 0.34941073\n",
      "Iteration 67, loss = 0.34947490\n",
      "Iteration 68, loss = 0.34870842\n",
      "Iteration 69, loss = 0.34816722\n",
      "Iteration 70, loss = 0.34743752\n",
      "Iteration 71, loss = 0.34625470\n",
      "Iteration 72, loss = 0.34595111\n",
      "Iteration 73, loss = 0.34347211\n",
      "Iteration 74, loss = 0.34391618\n",
      "Iteration 75, loss = 0.34304925\n",
      "Iteration 76, loss = 0.34194450\n",
      "Iteration 77, loss = 0.34036913\n",
      "Iteration 78, loss = 0.34052783\n",
      "Iteration 79, loss = 0.34119014\n",
      "Iteration 80, loss = 0.33950066\n",
      "Iteration 81, loss = 0.33851419\n",
      "Iteration 82, loss = 0.33818766\n",
      "Iteration 83, loss = 0.33727621\n",
      "Iteration 84, loss = 0.33626692\n",
      "Iteration 85, loss = 0.33570361\n",
      "Iteration 86, loss = 0.33605655\n",
      "Iteration 87, loss = 0.33516874\n",
      "Iteration 88, loss = 0.33371135\n",
      "Iteration 89, loss = 0.33287637\n",
      "Iteration 90, loss = 0.33387766\n",
      "Iteration 91, loss = 0.33159005\n",
      "Iteration 92, loss = 0.33091943\n",
      "Iteration 93, loss = 0.33069018\n",
      "Iteration 94, loss = 0.33017822\n",
      "Iteration 95, loss = 0.33142141\n",
      "Iteration 96, loss = 0.32920744\n",
      "Iteration 97, loss = 0.32925954\n",
      "Iteration 98, loss = 0.32849964\n",
      "Iteration 99, loss = 0.32683321\n",
      "Iteration 100, loss = 0.32607298\n",
      "Iteration 101, loss = 0.32787476\n",
      "Iteration 102, loss = 0.32560115\n",
      "Iteration 103, loss = 0.32577918\n",
      "Iteration 104, loss = 0.32530592\n",
      "Iteration 105, loss = 0.32567867\n",
      "Iteration 106, loss = 0.32378462\n",
      "Iteration 107, loss = 0.32255173\n",
      "Iteration 108, loss = 0.32367975\n",
      "Iteration 109, loss = 0.32264448\n",
      "Iteration 110, loss = 0.32256126\n",
      "Iteration 111, loss = 0.32317329\n",
      "Iteration 112, loss = 0.32181831\n",
      "Iteration 113, loss = 0.32003107\n",
      "Iteration 114, loss = 0.31969243\n",
      "Iteration 115, loss = 0.32056735\n",
      "Iteration 116, loss = 0.31960030\n",
      "Iteration 117, loss = 0.32129381\n",
      "Iteration 118, loss = 0.31797137\n",
      "Iteration 119, loss = 0.31782550\n",
      "Iteration 120, loss = 0.31755015\n",
      "Iteration 121, loss = 0.31880227\n",
      "Iteration 122, loss = 0.31723052\n",
      "Iteration 123, loss = 0.31605651\n",
      "Iteration 124, loss = 0.31774185\n",
      "Iteration 125, loss = 0.31378456\n",
      "Iteration 126, loss = 0.31401597\n",
      "Iteration 127, loss = 0.31497856\n",
      "Iteration 128, loss = 0.31271591\n",
      "Iteration 129, loss = 0.31493245\n",
      "Iteration 130, loss = 0.31469878\n",
      "Iteration 131, loss = 0.31362240\n",
      "Iteration 132, loss = 0.31332561\n",
      "Iteration 133, loss = 0.31092311\n",
      "Iteration 134, loss = 0.31304025\n",
      "Iteration 135, loss = 0.31384108\n",
      "Iteration 136, loss = 0.31045514\n",
      "Iteration 137, loss = 0.31203788\n",
      "Iteration 138, loss = 0.31122731\n",
      "Iteration 139, loss = 0.31186946\n",
      "Iteration 140, loss = 0.31039448\n",
      "Iteration 141, loss = 0.30913372\n",
      "Iteration 142, loss = 0.31020959\n",
      "Iteration 143, loss = 0.30954825\n",
      "Iteration 144, loss = 0.31068141\n",
      "Iteration 145, loss = 0.30822500\n",
      "Iteration 146, loss = 0.30822012\n",
      "Iteration 147, loss = 0.30745706\n",
      "Iteration 148, loss = 0.30913990\n",
      "Iteration 149, loss = 0.30764511\n",
      "Iteration 150, loss = 0.30749603\n",
      "Iteration 151, loss = 0.30800863\n",
      "Iteration 152, loss = 0.30654269\n",
      "Iteration 153, loss = 0.30708387\n",
      "Iteration 154, loss = 0.30668372\n",
      "Iteration 155, loss = 0.30400720\n",
      "Iteration 156, loss = 0.30532801\n",
      "Iteration 157, loss = 0.30559515\n",
      "Iteration 158, loss = 0.30587479\n",
      "Iteration 159, loss = 0.30441151\n",
      "Iteration 160, loss = 0.30425424\n",
      "Iteration 161, loss = 0.30350618\n",
      "Iteration 162, loss = 0.30438431\n",
      "Iteration 163, loss = 0.30384629\n",
      "Iteration 164, loss = 0.30346752\n",
      "Iteration 165, loss = 0.30268684\n",
      "Iteration 166, loss = 0.30242218\n",
      "Iteration 167, loss = 0.30530168\n",
      "Iteration 168, loss = 0.30112817\n",
      "Iteration 169, loss = 0.30069733\n",
      "Iteration 170, loss = 0.30242237\n",
      "Iteration 171, loss = 0.30000839\n",
      "Iteration 172, loss = 0.29939387\n",
      "Iteration 173, loss = 0.30046600\n",
      "Iteration 174, loss = 0.29993831\n",
      "Iteration 175, loss = 0.30065173\n",
      "Iteration 176, loss = 0.30055046\n",
      "Iteration 177, loss = 0.29969046\n",
      "Iteration 178, loss = 0.30010793\n",
      "Iteration 179, loss = 0.29959548\n",
      "Iteration 180, loss = 0.30056526\n",
      "Iteration 181, loss = 0.29830677\n",
      "Iteration 182, loss = 0.29952204\n",
      "Iteration 183, loss = 0.29752066\n",
      "Iteration 184, loss = 0.29865827\n",
      "Iteration 185, loss = 0.29772093\n",
      "Iteration 186, loss = 0.29925754\n",
      "Iteration 187, loss = 0.29740025\n",
      "Iteration 188, loss = 0.29727385\n",
      "Iteration 189, loss = 0.29794162\n",
      "Iteration 190, loss = 0.29583265\n",
      "Iteration 191, loss = 0.29673247\n",
      "Iteration 192, loss = 0.29591952\n",
      "Iteration 193, loss = 0.29612956\n",
      "Iteration 194, loss = 0.29620675\n",
      "Iteration 195, loss = 0.29600800\n",
      "Iteration 196, loss = 0.29546720\n",
      "Iteration 197, loss = 0.29651251\n",
      "Iteration 198, loss = 0.29584192\n",
      "Iteration 199, loss = 0.29437468\n",
      "Iteration 200, loss = 0.29316726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.53960479\n",
      "Iteration 2, loss = 0.51119307\n",
      "Iteration 3, loss = 0.50201773\n",
      "Iteration 4, loss = 0.49494509\n",
      "Iteration 5, loss = 0.48829354\n",
      "Iteration 6, loss = 0.48150398\n",
      "Iteration 7, loss = 0.47450606\n",
      "Iteration 8, loss = 0.46982416\n",
      "Iteration 9, loss = 0.46424586\n",
      "Iteration 10, loss = 0.45887760\n",
      "Iteration 11, loss = 0.45435209\n",
      "Iteration 12, loss = 0.44952022\n",
      "Iteration 13, loss = 0.44511304\n",
      "Iteration 14, loss = 0.44026309\n",
      "Iteration 15, loss = 0.43761047\n",
      "Iteration 16, loss = 0.43349338\n",
      "Iteration 17, loss = 0.43052280\n",
      "Iteration 18, loss = 0.42691763\n",
      "Iteration 19, loss = 0.42475873\n",
      "Iteration 20, loss = 0.42044560\n",
      "Iteration 21, loss = 0.41800500\n",
      "Iteration 22, loss = 0.41496783\n",
      "Iteration 23, loss = 0.41303965\n",
      "Iteration 24, loss = 0.40995290\n",
      "Iteration 25, loss = 0.40663282\n",
      "Iteration 26, loss = 0.40481484\n",
      "Iteration 27, loss = 0.40273104\n",
      "Iteration 28, loss = 0.40078816\n",
      "Iteration 29, loss = 0.39833759\n",
      "Iteration 30, loss = 0.39651535\n",
      "Iteration 31, loss = 0.39338834\n",
      "Iteration 32, loss = 0.39174498\n",
      "Iteration 33, loss = 0.39052132\n",
      "Iteration 34, loss = 0.38891023\n",
      "Iteration 35, loss = 0.38645496\n",
      "Iteration 36, loss = 0.38472495\n",
      "Iteration 37, loss = 0.38352370\n",
      "Iteration 38, loss = 0.38241993\n",
      "Iteration 39, loss = 0.38038052\n",
      "Iteration 40, loss = 0.37875755\n",
      "Iteration 41, loss = 0.37606304\n",
      "Iteration 42, loss = 0.37550599\n",
      "Iteration 43, loss = 0.37324501\n",
      "Iteration 44, loss = 0.37236019\n",
      "Iteration 45, loss = 0.37028580\n",
      "Iteration 46, loss = 0.37014750\n",
      "Iteration 47, loss = 0.36862006\n",
      "Iteration 48, loss = 0.36609086\n",
      "Iteration 49, loss = 0.36610481\n",
      "Iteration 50, loss = 0.36311128\n",
      "Iteration 51, loss = 0.36350474\n",
      "Iteration 52, loss = 0.36092937\n",
      "Iteration 53, loss = 0.36124841\n",
      "Iteration 54, loss = 0.35930612\n",
      "Iteration 55, loss = 0.35752745\n",
      "Iteration 56, loss = 0.35672955\n",
      "Iteration 57, loss = 0.35696930\n",
      "Iteration 58, loss = 0.35511403\n",
      "Iteration 59, loss = 0.35307004\n",
      "Iteration 60, loss = 0.35245878\n",
      "Iteration 61, loss = 0.35199941\n",
      "Iteration 62, loss = 0.35178474\n",
      "Iteration 63, loss = 0.34871378\n",
      "Iteration 64, loss = 0.34950839\n",
      "Iteration 65, loss = 0.34889210\n",
      "Iteration 66, loss = 0.34809439\n",
      "Iteration 67, loss = 0.34654612\n",
      "Iteration 68, loss = 0.34534050\n",
      "Iteration 69, loss = 0.34472642\n",
      "Iteration 70, loss = 0.34457162\n",
      "Iteration 71, loss = 0.34198695\n",
      "Iteration 72, loss = 0.34143838\n",
      "Iteration 73, loss = 0.34138595\n",
      "Iteration 74, loss = 0.33951848\n",
      "Iteration 75, loss = 0.34019620\n",
      "Iteration 76, loss = 0.33783066\n",
      "Iteration 77, loss = 0.33816947\n",
      "Iteration 78, loss = 0.33728870\n",
      "Iteration 79, loss = 0.33585875\n",
      "Iteration 80, loss = 0.33487784\n",
      "Iteration 81, loss = 0.33616036\n",
      "Iteration 82, loss = 0.33417885\n",
      "Iteration 83, loss = 0.33398775\n",
      "Iteration 84, loss = 0.33393266\n",
      "Iteration 85, loss = 0.33040935\n",
      "Iteration 86, loss = 0.33103492\n",
      "Iteration 87, loss = 0.32999359\n",
      "Iteration 88, loss = 0.33045687\n",
      "Iteration 89, loss = 0.32791858\n",
      "Iteration 90, loss = 0.32916235\n",
      "Iteration 91, loss = 0.32619471\n",
      "Iteration 92, loss = 0.32681246\n",
      "Iteration 93, loss = 0.32831165\n",
      "Iteration 94, loss = 0.32565119\n",
      "Iteration 95, loss = 0.32579084\n",
      "Iteration 96, loss = 0.32581974\n",
      "Iteration 97, loss = 0.32652980\n",
      "Iteration 98, loss = 0.32371856\n",
      "Iteration 99, loss = 0.32354375\n",
      "Iteration 100, loss = 0.32591192\n",
      "Iteration 101, loss = 0.32256545\n",
      "Iteration 102, loss = 0.32226128\n",
      "Iteration 103, loss = 0.32155801\n",
      "Iteration 104, loss = 0.32097092\n",
      "Iteration 105, loss = 0.31976040\n",
      "Iteration 106, loss = 0.32083094\n",
      "Iteration 107, loss = 0.31902412\n",
      "Iteration 108, loss = 0.32152304\n",
      "Iteration 109, loss = 0.31706816\n",
      "Iteration 110, loss = 0.31776088\n",
      "Iteration 111, loss = 0.31780732\n",
      "Iteration 112, loss = 0.31831523\n",
      "Iteration 113, loss = 0.31697029\n",
      "Iteration 114, loss = 0.31555737\n",
      "Iteration 115, loss = 0.31555298\n",
      "Iteration 116, loss = 0.31587651\n",
      "Iteration 117, loss = 0.31544843\n",
      "Iteration 118, loss = 0.31274479\n",
      "Iteration 119, loss = 0.31239390\n",
      "Iteration 120, loss = 0.31339969\n",
      "Iteration 121, loss = 0.31257112\n",
      "Iteration 122, loss = 0.31177324\n",
      "Iteration 123, loss = 0.31167993\n",
      "Iteration 124, loss = 0.31265736\n",
      "Iteration 125, loss = 0.31131068\n",
      "Iteration 126, loss = 0.31042709\n",
      "Iteration 127, loss = 0.31089393\n",
      "Iteration 128, loss = 0.31338841\n",
      "Iteration 129, loss = 0.30978850\n",
      "Iteration 130, loss = 0.30810869\n",
      "Iteration 131, loss = 0.30795708\n",
      "Iteration 132, loss = 0.30710824\n",
      "Iteration 133, loss = 0.30905811\n",
      "Iteration 134, loss = 0.30849275\n",
      "Iteration 135, loss = 0.30706132\n",
      "Iteration 136, loss = 0.30603480\n",
      "Iteration 137, loss = 0.30790721\n",
      "Iteration 138, loss = 0.30615492\n",
      "Iteration 139, loss = 0.30636790\n",
      "Iteration 140, loss = 0.30659940\n",
      "Iteration 141, loss = 0.30620657\n",
      "Iteration 142, loss = 0.30467309\n",
      "Iteration 143, loss = 0.30491023\n",
      "Iteration 144, loss = 0.30414083\n",
      "Iteration 145, loss = 0.30279247\n",
      "Iteration 146, loss = 0.30305487\n",
      "Iteration 147, loss = 0.30326801\n",
      "Iteration 148, loss = 0.30432430\n",
      "Iteration 149, loss = 0.30208992\n",
      "Iteration 150, loss = 0.30140344\n",
      "Iteration 151, loss = 0.30421169\n",
      "Iteration 152, loss = 0.30230671\n",
      "Iteration 153, loss = 0.30252984\n",
      "Iteration 154, loss = 0.30335535\n",
      "Iteration 155, loss = 0.30017934\n",
      "Iteration 156, loss = 0.30069274\n",
      "Iteration 157, loss = 0.30055538\n",
      "Iteration 158, loss = 0.29881211\n",
      "Iteration 159, loss = 0.29883611\n",
      "Iteration 160, loss = 0.29942227\n",
      "Iteration 161, loss = 0.30108384\n",
      "Iteration 162, loss = 0.29852457\n",
      "Iteration 163, loss = 0.29783831\n",
      "Iteration 164, loss = 0.29931532\n",
      "Iteration 165, loss = 0.29851307\n",
      "Iteration 166, loss = 0.29781624\n",
      "Iteration 167, loss = 0.29750214\n",
      "Iteration 168, loss = 0.29512157\n",
      "Iteration 169, loss = 0.29888366\n",
      "Iteration 170, loss = 0.29688892\n",
      "Iteration 171, loss = 0.29687900\n",
      "Iteration 172, loss = 0.29638281\n",
      "Iteration 173, loss = 0.29535784\n",
      "Iteration 174, loss = 0.29692108\n",
      "Iteration 175, loss = 0.29607618\n",
      "Iteration 176, loss = 0.29656725\n",
      "Iteration 177, loss = 0.29626364\n",
      "Iteration 178, loss = 0.29694415\n",
      "Iteration 179, loss = 0.29387895\n",
      "Iteration 180, loss = 0.29509197\n",
      "Iteration 181, loss = 0.29382097\n",
      "Iteration 182, loss = 0.29328530\n",
      "Iteration 183, loss = 0.29421649\n",
      "Iteration 184, loss = 0.29355562\n",
      "Iteration 185, loss = 0.29316545\n",
      "Iteration 186, loss = 0.29371012\n",
      "Iteration 187, loss = 0.29413632\n",
      "Iteration 188, loss = 0.29157165\n",
      "Iteration 189, loss = 0.29265983\n",
      "Iteration 190, loss = 0.29336962\n",
      "Iteration 191, loss = 0.29310791\n",
      "Iteration 192, loss = 0.29185121\n",
      "Iteration 193, loss = 0.29071287\n",
      "Iteration 194, loss = 0.29215074\n",
      "Iteration 195, loss = 0.29080117\n",
      "Iteration 196, loss = 0.29069940\n",
      "Iteration 197, loss = 0.29056449\n",
      "Iteration 198, loss = 0.28953546\n",
      "Iteration 199, loss = 0.28959849\n",
      "Iteration 200, loss = 0.29084863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.53896562\n",
      "Iteration 2, loss = 0.51068434\n",
      "Iteration 3, loss = 0.50077809\n",
      "Iteration 4, loss = 0.49327300\n",
      "Iteration 5, loss = 0.48584723\n",
      "Iteration 6, loss = 0.47980421\n",
      "Iteration 7, loss = 0.47395206\n",
      "Iteration 8, loss = 0.46873082\n",
      "Iteration 9, loss = 0.46286767\n",
      "Iteration 10, loss = 0.45746472\n",
      "Iteration 11, loss = 0.45309122\n",
      "Iteration 12, loss = 0.44884046\n",
      "Iteration 13, loss = 0.44442956\n",
      "Iteration 14, loss = 0.44020457\n",
      "Iteration 15, loss = 0.43639249\n",
      "Iteration 16, loss = 0.43275364\n",
      "Iteration 17, loss = 0.42909818\n",
      "Iteration 18, loss = 0.42659675\n",
      "Iteration 19, loss = 0.42240362\n",
      "Iteration 20, loss = 0.42085264\n",
      "Iteration 21, loss = 0.41733558\n",
      "Iteration 22, loss = 0.41453466\n",
      "Iteration 23, loss = 0.41171355\n",
      "Iteration 24, loss = 0.40915203\n",
      "Iteration 25, loss = 0.40735600\n",
      "Iteration 26, loss = 0.40452785\n",
      "Iteration 27, loss = 0.40031806\n",
      "Iteration 28, loss = 0.39956221\n",
      "Iteration 29, loss = 0.39841193\n",
      "Iteration 30, loss = 0.39487713\n",
      "Iteration 31, loss = 0.39332099\n",
      "Iteration 32, loss = 0.39200687\n",
      "Iteration 33, loss = 0.38950696\n",
      "Iteration 34, loss = 0.38810522\n",
      "Iteration 35, loss = 0.38559899\n",
      "Iteration 36, loss = 0.38484199\n",
      "Iteration 37, loss = 0.38154837\n",
      "Iteration 38, loss = 0.38140109\n",
      "Iteration 39, loss = 0.37949899\n",
      "Iteration 40, loss = 0.37723279\n",
      "Iteration 41, loss = 0.37525933\n",
      "Iteration 42, loss = 0.37477358\n",
      "Iteration 43, loss = 0.37192637\n",
      "Iteration 44, loss = 0.37140838\n",
      "Iteration 45, loss = 0.36978054\n",
      "Iteration 46, loss = 0.36879401\n",
      "Iteration 47, loss = 0.36746555\n",
      "Iteration 48, loss = 0.36753479\n",
      "Iteration 49, loss = 0.36508037\n",
      "Iteration 50, loss = 0.36344255\n",
      "Iteration 51, loss = 0.36238013\n",
      "Iteration 52, loss = 0.36065863\n",
      "Iteration 53, loss = 0.36049028\n",
      "Iteration 54, loss = 0.35892799\n",
      "Iteration 55, loss = 0.35770778\n",
      "Iteration 56, loss = 0.35612988\n",
      "Iteration 57, loss = 0.35551437\n",
      "Iteration 58, loss = 0.35334846\n",
      "Iteration 59, loss = 0.35451196\n",
      "Iteration 60, loss = 0.35196054\n",
      "Iteration 61, loss = 0.35207568\n",
      "Iteration 62, loss = 0.34972446\n",
      "Iteration 63, loss = 0.34929418\n",
      "Iteration 64, loss = 0.34759159\n",
      "Iteration 65, loss = 0.34650127\n",
      "Iteration 66, loss = 0.34619124\n",
      "Iteration 67, loss = 0.34647383\n",
      "Iteration 68, loss = 0.34460175\n",
      "Iteration 69, loss = 0.34402886\n",
      "Iteration 70, loss = 0.34349735\n",
      "Iteration 71, loss = 0.34207558\n",
      "Iteration 72, loss = 0.34109493\n",
      "Iteration 73, loss = 0.34027894\n",
      "Iteration 74, loss = 0.33998150\n",
      "Iteration 75, loss = 0.33961409\n",
      "Iteration 76, loss = 0.33956183\n",
      "Iteration 77, loss = 0.33827615\n",
      "Iteration 78, loss = 0.33818017\n",
      "Iteration 79, loss = 0.33762920\n",
      "Iteration 80, loss = 0.33766615\n",
      "Iteration 81, loss = 0.33630674\n",
      "Iteration 82, loss = 0.33451910\n",
      "Iteration 83, loss = 0.33300058\n",
      "Iteration 84, loss = 0.33341649\n",
      "Iteration 85, loss = 0.33466158\n",
      "Iteration 86, loss = 0.33255383\n",
      "Iteration 87, loss = 0.33049443\n",
      "Iteration 88, loss = 0.33070043\n",
      "Iteration 89, loss = 0.33015520\n",
      "Iteration 90, loss = 0.33033729\n",
      "Iteration 91, loss = 0.33162239\n",
      "Iteration 92, loss = 0.32791611\n",
      "Iteration 93, loss = 0.32729679\n",
      "Iteration 94, loss = 0.32723304\n",
      "Iteration 95, loss = 0.32800196\n",
      "Iteration 96, loss = 0.32594093\n",
      "Iteration 97, loss = 0.32851985\n",
      "Iteration 98, loss = 0.32487807\n",
      "Iteration 99, loss = 0.32447173\n",
      "Iteration 100, loss = 0.32290299\n",
      "Iteration 101, loss = 0.32369069\n",
      "Iteration 102, loss = 0.32333253\n",
      "Iteration 103, loss = 0.32394059\n",
      "Iteration 104, loss = 0.32295883\n",
      "Iteration 105, loss = 0.32123635\n",
      "Iteration 106, loss = 0.32069754\n",
      "Iteration 107, loss = 0.31989907\n",
      "Iteration 108, loss = 0.32220676\n",
      "Iteration 109, loss = 0.31942412\n",
      "Iteration 110, loss = 0.31866753\n",
      "Iteration 111, loss = 0.31920103\n",
      "Iteration 112, loss = 0.31887863\n",
      "Iteration 113, loss = 0.31769656\n",
      "Iteration 114, loss = 0.31821911\n",
      "Iteration 115, loss = 0.31768044\n",
      "Iteration 116, loss = 0.31742187\n",
      "Iteration 117, loss = 0.31717348\n",
      "Iteration 118, loss = 0.31614955\n",
      "Iteration 119, loss = 0.31598248\n",
      "Iteration 120, loss = 0.31592081\n",
      "Iteration 121, loss = 0.31516490\n",
      "Iteration 122, loss = 0.31491018\n",
      "Iteration 123, loss = 0.31677043\n",
      "Iteration 124, loss = 0.31367066\n",
      "Iteration 125, loss = 0.31271847\n",
      "Iteration 126, loss = 0.31275795\n",
      "Iteration 127, loss = 0.31374873\n",
      "Iteration 128, loss = 0.31207859\n",
      "Iteration 129, loss = 0.31219581\n",
      "Iteration 130, loss = 0.31224240\n",
      "Iteration 131, loss = 0.30986330\n",
      "Iteration 132, loss = 0.31108862\n",
      "Iteration 133, loss = 0.31140748\n",
      "Iteration 134, loss = 0.31000986\n",
      "Iteration 135, loss = 0.30978970\n",
      "Iteration 136, loss = 0.30960490\n",
      "Iteration 137, loss = 0.30951961\n",
      "Iteration 138, loss = 0.30918729\n",
      "Iteration 139, loss = 0.30889461\n",
      "Iteration 140, loss = 0.30851454\n",
      "Iteration 141, loss = 0.30771650\n",
      "Iteration 142, loss = 0.30828277\n",
      "Iteration 143, loss = 0.30863347\n",
      "Iteration 144, loss = 0.30734369\n",
      "Iteration 145, loss = 0.30546808\n",
      "Iteration 146, loss = 0.30625640\n",
      "Iteration 147, loss = 0.30645074\n",
      "Iteration 148, loss = 0.30582105\n",
      "Iteration 149, loss = 0.30501140\n",
      "Iteration 150, loss = 0.30528702\n",
      "Iteration 151, loss = 0.30631674\n",
      "Iteration 152, loss = 0.30598665\n",
      "Iteration 153, loss = 0.30271476\n",
      "Iteration 154, loss = 0.30446303\n",
      "Iteration 155, loss = 0.30362199\n",
      "Iteration 156, loss = 0.30490722\n",
      "Iteration 157, loss = 0.30376407\n",
      "Iteration 158, loss = 0.30387303\n",
      "Iteration 159, loss = 0.30243601\n",
      "Iteration 160, loss = 0.30350375\n",
      "Iteration 161, loss = 0.30108927\n",
      "Iteration 162, loss = 0.30322769\n",
      "Iteration 163, loss = 0.30121126\n",
      "Iteration 164, loss = 0.30160028\n",
      "Iteration 165, loss = 0.30195889\n",
      "Iteration 166, loss = 0.30003380\n",
      "Iteration 167, loss = 0.30087995\n",
      "Iteration 168, loss = 0.30123471\n",
      "Iteration 169, loss = 0.30060916\n",
      "Iteration 170, loss = 0.30036424\n",
      "Iteration 171, loss = 0.30103287\n",
      "Iteration 172, loss = 0.29915633\n",
      "Iteration 173, loss = 0.29872346\n",
      "Iteration 174, loss = 0.29933826\n",
      "Iteration 175, loss = 0.29883152\n",
      "Iteration 176, loss = 0.29920878\n",
      "Iteration 177, loss = 0.29730953\n",
      "Iteration 178, loss = 0.29883906\n",
      "Iteration 179, loss = 0.29737318\n",
      "Iteration 180, loss = 0.29767181\n",
      "Iteration 181, loss = 0.29687065\n",
      "Iteration 182, loss = 0.29799085\n",
      "Iteration 183, loss = 0.29747088\n",
      "Iteration 184, loss = 0.29959031\n",
      "Iteration 185, loss = 0.29779586\n",
      "Iteration 186, loss = 0.29717470\n",
      "Iteration 187, loss = 0.29641797\n",
      "Iteration 188, loss = 0.29394672\n",
      "Iteration 189, loss = 0.29790622\n",
      "Iteration 190, loss = 0.29543005\n",
      "Iteration 191, loss = 0.29739659\n",
      "Iteration 192, loss = 0.29603657\n",
      "Iteration 193, loss = 0.29479157\n",
      "Iteration 194, loss = 0.29613816\n",
      "Iteration 195, loss = 0.29526458\n",
      "Iteration 196, loss = 0.29445483\n",
      "Iteration 197, loss = 0.29505423\n",
      "Iteration 198, loss = 0.29341165\n",
      "Iteration 199, loss = 0.29332653\n",
      "Iteration 200, loss = 0.29364231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.53762197\n",
      "Iteration 2, loss = 0.51193171\n",
      "Iteration 3, loss = 0.50219328\n",
      "Iteration 4, loss = 0.49430040\n",
      "Iteration 5, loss = 0.48784656\n",
      "Iteration 6, loss = 0.48187224\n",
      "Iteration 7, loss = 0.47530505\n",
      "Iteration 8, loss = 0.47018599\n",
      "Iteration 9, loss = 0.46505209\n",
      "Iteration 10, loss = 0.45992378\n",
      "Iteration 11, loss = 0.45550887\n",
      "Iteration 12, loss = 0.45120361\n",
      "Iteration 13, loss = 0.44628088\n",
      "Iteration 14, loss = 0.44211017\n",
      "Iteration 15, loss = 0.43862264\n",
      "Iteration 16, loss = 0.43511621\n",
      "Iteration 17, loss = 0.43163698\n",
      "Iteration 18, loss = 0.42865057\n",
      "Iteration 19, loss = 0.42623144\n",
      "Iteration 20, loss = 0.42244525\n",
      "Iteration 21, loss = 0.41988304\n",
      "Iteration 22, loss = 0.41844599\n",
      "Iteration 23, loss = 0.41574612\n",
      "Iteration 24, loss = 0.41226434\n",
      "Iteration 25, loss = 0.40993382\n",
      "Iteration 26, loss = 0.40755620\n",
      "Iteration 27, loss = 0.40568244\n",
      "Iteration 28, loss = 0.40291740\n",
      "Iteration 29, loss = 0.40136935\n",
      "Iteration 30, loss = 0.39865612\n",
      "Iteration 31, loss = 0.39703935\n",
      "Iteration 32, loss = 0.39396627\n",
      "Iteration 33, loss = 0.39362355\n",
      "Iteration 34, loss = 0.39226983\n",
      "Iteration 35, loss = 0.38969997\n",
      "Iteration 36, loss = 0.38683378\n",
      "Iteration 37, loss = 0.38674130\n",
      "Iteration 38, loss = 0.38395305\n",
      "Iteration 39, loss = 0.38250169\n",
      "Iteration 40, loss = 0.38059105\n",
      "Iteration 41, loss = 0.38015624\n",
      "Iteration 42, loss = 0.37784467\n",
      "Iteration 43, loss = 0.37510213\n",
      "Iteration 44, loss = 0.37571862\n",
      "Iteration 45, loss = 0.37402002\n",
      "Iteration 46, loss = 0.37333200\n",
      "Iteration 47, loss = 0.37037437\n",
      "Iteration 48, loss = 0.36806964\n",
      "Iteration 49, loss = 0.36903762\n",
      "Iteration 50, loss = 0.36600869\n",
      "Iteration 51, loss = 0.36580731\n",
      "Iteration 52, loss = 0.36425937\n",
      "Iteration 53, loss = 0.36350887\n",
      "Iteration 54, loss = 0.36165391\n",
      "Iteration 55, loss = 0.36109497\n",
      "Iteration 56, loss = 0.35889957\n",
      "Iteration 57, loss = 0.35827978\n",
      "Iteration 58, loss = 0.35721933\n",
      "Iteration 59, loss = 0.35607205\n",
      "Iteration 60, loss = 0.35490253\n",
      "Iteration 61, loss = 0.35370475\n",
      "Iteration 62, loss = 0.35390653\n",
      "Iteration 63, loss = 0.35243228\n",
      "Iteration 64, loss = 0.35276268\n",
      "Iteration 65, loss = 0.35079467\n",
      "Iteration 66, loss = 0.34958159\n",
      "Iteration 67, loss = 0.34963296\n",
      "Iteration 68, loss = 0.34791278\n",
      "Iteration 69, loss = 0.34744762\n",
      "Iteration 70, loss = 0.34612636\n",
      "Iteration 71, loss = 0.34485448\n",
      "Iteration 72, loss = 0.34421801\n",
      "Iteration 73, loss = 0.34341218\n",
      "Iteration 74, loss = 0.34139134\n",
      "Iteration 75, loss = 0.34193956\n",
      "Iteration 76, loss = 0.34233197\n",
      "Iteration 77, loss = 0.33940418\n",
      "Iteration 78, loss = 0.33904372\n",
      "Iteration 79, loss = 0.33928180\n",
      "Iteration 80, loss = 0.33808041\n",
      "Iteration 81, loss = 0.33591031\n",
      "Iteration 82, loss = 0.33662317\n",
      "Iteration 83, loss = 0.33720043\n",
      "Iteration 84, loss = 0.33616401\n",
      "Iteration 85, loss = 0.33450204\n",
      "Iteration 86, loss = 0.33309091\n",
      "Iteration 87, loss = 0.33454540\n",
      "Iteration 88, loss = 0.33226263\n",
      "Iteration 89, loss = 0.33145735\n",
      "Iteration 90, loss = 0.33107271\n",
      "Iteration 91, loss = 0.32962720\n",
      "Iteration 92, loss = 0.32928562\n",
      "Iteration 93, loss = 0.32883995\n",
      "Iteration 94, loss = 0.32875177\n",
      "Iteration 95, loss = 0.32817974\n",
      "Iteration 96, loss = 0.32730192\n",
      "Iteration 97, loss = 0.32760233\n",
      "Iteration 98, loss = 0.32679891\n",
      "Iteration 99, loss = 0.32653615\n",
      "Iteration 100, loss = 0.32563039\n",
      "Iteration 101, loss = 0.32651586\n",
      "Iteration 102, loss = 0.32600651\n",
      "Iteration 103, loss = 0.32461479\n",
      "Iteration 104, loss = 0.32443211\n",
      "Iteration 105, loss = 0.32300681\n",
      "Iteration 106, loss = 0.32241369\n",
      "Iteration 107, loss = 0.32146195\n",
      "Iteration 108, loss = 0.32129457\n",
      "Iteration 109, loss = 0.32199493\n",
      "Iteration 110, loss = 0.32114345\n",
      "Iteration 111, loss = 0.31984611\n",
      "Iteration 112, loss = 0.31964166\n",
      "Iteration 113, loss = 0.31811237\n",
      "Iteration 114, loss = 0.31867393\n",
      "Iteration 115, loss = 0.31868900\n",
      "Iteration 116, loss = 0.31890447\n",
      "Iteration 117, loss = 0.31766239\n",
      "Iteration 118, loss = 0.31646521\n",
      "Iteration 119, loss = 0.31565442\n",
      "Iteration 120, loss = 0.31536195\n",
      "Iteration 121, loss = 0.31645874\n",
      "Iteration 122, loss = 0.31567996\n",
      "Iteration 123, loss = 0.31391881\n",
      "Iteration 124, loss = 0.31453883\n",
      "Iteration 125, loss = 0.31459676\n",
      "Iteration 126, loss = 0.31401924\n",
      "Iteration 127, loss = 0.31307376\n",
      "Iteration 128, loss = 0.31393398\n",
      "Iteration 129, loss = 0.31346490\n",
      "Iteration 130, loss = 0.31189980\n",
      "Iteration 131, loss = 0.31240860\n",
      "Iteration 132, loss = 0.31136199\n",
      "Iteration 133, loss = 0.31274219\n",
      "Iteration 134, loss = 0.31016113\n",
      "Iteration 135, loss = 0.31017792\n",
      "Iteration 136, loss = 0.30995891\n",
      "Iteration 137, loss = 0.30947913\n",
      "Iteration 138, loss = 0.30821370\n",
      "Iteration 139, loss = 0.30758133\n",
      "Iteration 140, loss = 0.30770589\n",
      "Iteration 141, loss = 0.30845153\n",
      "Iteration 142, loss = 0.30926769\n",
      "Iteration 143, loss = 0.30677407\n",
      "Iteration 144, loss = 0.30538554\n",
      "Iteration 145, loss = 0.30590981\n",
      "Iteration 146, loss = 0.30685790\n",
      "Iteration 147, loss = 0.30627777\n",
      "Iteration 148, loss = 0.30513651\n",
      "Iteration 149, loss = 0.30519271\n",
      "Iteration 150, loss = 0.30559670\n",
      "Iteration 151, loss = 0.30460728\n",
      "Iteration 152, loss = 0.30417077\n",
      "Iteration 153, loss = 0.30416707\n",
      "Iteration 154, loss = 0.30249767\n",
      "Iteration 155, loss = 0.30350425\n",
      "Iteration 156, loss = 0.30202229\n",
      "Iteration 157, loss = 0.30141963\n",
      "Iteration 158, loss = 0.30270800\n",
      "Iteration 159, loss = 0.30275623\n",
      "Iteration 160, loss = 0.30155925\n",
      "Iteration 161, loss = 0.30030282\n",
      "Iteration 162, loss = 0.30144726\n",
      "Iteration 163, loss = 0.30031393\n",
      "Iteration 164, loss = 0.30016927\n",
      "Iteration 165, loss = 0.29957521\n",
      "Iteration 166, loss = 0.30153261\n",
      "Iteration 167, loss = 0.30030444\n",
      "Iteration 168, loss = 0.29978751\n",
      "Iteration 169, loss = 0.30043672\n",
      "Iteration 170, loss = 0.29905082\n",
      "Iteration 171, loss = 0.29822716\n",
      "Iteration 172, loss = 0.29790874\n",
      "Iteration 173, loss = 0.29911754\n",
      "Iteration 174, loss = 0.29734488\n",
      "Iteration 175, loss = 0.29711206\n",
      "Iteration 176, loss = 0.29719411\n",
      "Iteration 177, loss = 0.29736630\n",
      "Iteration 178, loss = 0.29725999\n",
      "Iteration 179, loss = 0.29534025\n",
      "Iteration 180, loss = 0.29558522\n",
      "Iteration 181, loss = 0.29705349\n",
      "Iteration 182, loss = 0.29649384\n",
      "Iteration 183, loss = 0.29580577\n",
      "Iteration 184, loss = 0.29615560\n",
      "Iteration 185, loss = 0.29323625\n",
      "Iteration 186, loss = 0.29630775\n",
      "Iteration 187, loss = 0.29388416\n",
      "Iteration 188, loss = 0.29623914\n",
      "Iteration 189, loss = 0.29365846\n",
      "Iteration 190, loss = 0.29499070\n",
      "Iteration 191, loss = 0.29350239\n",
      "Iteration 192, loss = 0.29495433\n",
      "Iteration 193, loss = 0.29228434\n",
      "Iteration 194, loss = 0.29426088\n",
      "Iteration 195, loss = 0.29307797\n",
      "Iteration 196, loss = 0.29209737\n",
      "Iteration 197, loss = 0.29240220\n",
      "Iteration 198, loss = 0.29083081\n",
      "Iteration 199, loss = 0.29260837\n",
      "Iteration 200, loss = 0.29112360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.53713141\n",
      "Iteration 2, loss = 0.50914291\n",
      "Iteration 3, loss = 0.50026455\n",
      "Iteration 4, loss = 0.49171351\n",
      "Iteration 5, loss = 0.48472856\n",
      "Iteration 6, loss = 0.47831674\n",
      "Iteration 7, loss = 0.47275039\n",
      "Iteration 8, loss = 0.46710902\n",
      "Iteration 9, loss = 0.46134576\n",
      "Iteration 10, loss = 0.45653481\n",
      "Iteration 11, loss = 0.45248056\n",
      "Iteration 12, loss = 0.44734362\n",
      "Iteration 13, loss = 0.44481738\n",
      "Iteration 14, loss = 0.43948893\n",
      "Iteration 15, loss = 0.43777103\n",
      "Iteration 16, loss = 0.43142872\n",
      "Iteration 17, loss = 0.42916194\n",
      "Iteration 18, loss = 0.42638169\n",
      "Iteration 19, loss = 0.42289267\n",
      "Iteration 20, loss = 0.41972278\n",
      "Iteration 21, loss = 0.41688789\n",
      "Iteration 22, loss = 0.41486232\n",
      "Iteration 23, loss = 0.41124470\n",
      "Iteration 24, loss = 0.40999451\n",
      "Iteration 25, loss = 0.40628538\n",
      "Iteration 26, loss = 0.40503318\n",
      "Iteration 27, loss = 0.40194007\n",
      "Iteration 28, loss = 0.39982452\n",
      "Iteration 29, loss = 0.39853114\n",
      "Iteration 30, loss = 0.39554315\n",
      "Iteration 31, loss = 0.39454960\n",
      "Iteration 32, loss = 0.39152578\n",
      "Iteration 33, loss = 0.38910181\n",
      "Iteration 34, loss = 0.38874135\n",
      "Iteration 35, loss = 0.38677742\n",
      "Iteration 36, loss = 0.38402134\n",
      "Iteration 37, loss = 0.38324617\n",
      "Iteration 38, loss = 0.38099861\n",
      "Iteration 39, loss = 0.37922274\n",
      "Iteration 40, loss = 0.37958003\n",
      "Iteration 41, loss = 0.37695101\n",
      "Iteration 42, loss = 0.37537228\n",
      "Iteration 43, loss = 0.37322554\n",
      "Iteration 44, loss = 0.37403568\n",
      "Iteration 45, loss = 0.37078843\n",
      "Iteration 46, loss = 0.36897740\n",
      "Iteration 47, loss = 0.36915439\n",
      "Iteration 48, loss = 0.36633257\n",
      "Iteration 49, loss = 0.36527146\n",
      "Iteration 50, loss = 0.36371561\n",
      "Iteration 51, loss = 0.36175054\n",
      "Iteration 52, loss = 0.36188081\n",
      "Iteration 53, loss = 0.36117725\n",
      "Iteration 54, loss = 0.35948819\n",
      "Iteration 55, loss = 0.35689682\n",
      "Iteration 56, loss = 0.35721609\n",
      "Iteration 57, loss = 0.35501156\n",
      "Iteration 58, loss = 0.35582818\n",
      "Iteration 59, loss = 0.35443415\n",
      "Iteration 60, loss = 0.35302400\n",
      "Iteration 61, loss = 0.35169180\n",
      "Iteration 62, loss = 0.35079538\n",
      "Iteration 63, loss = 0.35015876\n",
      "Iteration 64, loss = 0.34976279\n",
      "Iteration 65, loss = 0.34707490\n",
      "Iteration 66, loss = 0.34754503\n",
      "Iteration 67, loss = 0.34750255\n",
      "Iteration 68, loss = 0.34503109\n",
      "Iteration 69, loss = 0.34464619\n",
      "Iteration 70, loss = 0.34462810\n",
      "Iteration 71, loss = 0.34302413\n",
      "Iteration 72, loss = 0.34241624\n",
      "Iteration 73, loss = 0.34223166\n",
      "Iteration 74, loss = 0.34042802\n",
      "Iteration 75, loss = 0.33923976\n",
      "Iteration 76, loss = 0.33917684\n",
      "Iteration 77, loss = 0.33806013\n",
      "Iteration 78, loss = 0.33811774\n",
      "Iteration 79, loss = 0.33713774\n",
      "Iteration 80, loss = 0.33680178\n",
      "Iteration 81, loss = 0.33547312\n",
      "Iteration 82, loss = 0.33412192\n",
      "Iteration 83, loss = 0.33392267\n",
      "Iteration 84, loss = 0.33309901\n",
      "Iteration 85, loss = 0.33308782\n",
      "Iteration 86, loss = 0.33261169\n",
      "Iteration 87, loss = 0.33287865\n",
      "Iteration 88, loss = 0.33015324\n",
      "Iteration 89, loss = 0.33025558\n",
      "Iteration 90, loss = 0.33057312\n",
      "Iteration 91, loss = 0.32857525\n",
      "Iteration 92, loss = 0.32894407\n",
      "Iteration 93, loss = 0.32854902\n",
      "Iteration 94, loss = 0.32725454\n",
      "Iteration 95, loss = 0.32715134\n",
      "Iteration 96, loss = 0.32592198\n",
      "Iteration 97, loss = 0.32511266\n",
      "Iteration 98, loss = 0.32532922\n",
      "Iteration 99, loss = 0.32705555\n",
      "Iteration 100, loss = 0.32340395\n",
      "Iteration 101, loss = 0.32492996\n",
      "Iteration 102, loss = 0.32299543\n",
      "Iteration 103, loss = 0.32147126\n",
      "Iteration 104, loss = 0.32190999\n",
      "Iteration 105, loss = 0.32183434\n",
      "Iteration 106, loss = 0.31975154\n",
      "Iteration 107, loss = 0.32050236\n",
      "Iteration 108, loss = 0.32026038\n",
      "Iteration 109, loss = 0.31901556\n",
      "Iteration 110, loss = 0.31847384\n",
      "Iteration 111, loss = 0.32113069\n",
      "Iteration 112, loss = 0.31802967\n",
      "Iteration 113, loss = 0.32008791\n",
      "Iteration 114, loss = 0.31822720\n",
      "Iteration 115, loss = 0.31560409\n",
      "Iteration 116, loss = 0.31556152\n",
      "Iteration 117, loss = 0.31458577\n",
      "Iteration 118, loss = 0.31624880\n",
      "Iteration 119, loss = 0.31511142\n",
      "Iteration 120, loss = 0.31578442\n",
      "Iteration 121, loss = 0.31417583\n",
      "Iteration 122, loss = 0.31401187\n",
      "Iteration 123, loss = 0.31264185\n",
      "Iteration 124, loss = 0.31366931\n",
      "Iteration 125, loss = 0.31247884\n",
      "Iteration 126, loss = 0.31134838\n",
      "Iteration 127, loss = 0.31116661\n",
      "Iteration 128, loss = 0.31089096\n",
      "Iteration 129, loss = 0.31239435\n",
      "Iteration 130, loss = 0.31128797\n",
      "Iteration 131, loss = 0.30924784\n",
      "Iteration 132, loss = 0.30921457\n",
      "Iteration 133, loss = 0.30916971\n",
      "Iteration 134, loss = 0.31057555\n",
      "Iteration 135, loss = 0.30935372\n",
      "Iteration 136, loss = 0.30832572\n",
      "Iteration 137, loss = 0.30761649\n",
      "Iteration 138, loss = 0.30873515\n",
      "Iteration 139, loss = 0.30675898\n",
      "Iteration 140, loss = 0.30770244\n",
      "Iteration 141, loss = 0.30814924\n",
      "Iteration 142, loss = 0.30898914\n",
      "Iteration 143, loss = 0.30656683\n",
      "Iteration 144, loss = 0.30522155\n",
      "Iteration 145, loss = 0.30656721\n",
      "Iteration 146, loss = 0.30644104\n",
      "Iteration 147, loss = 0.30669993\n",
      "Iteration 148, loss = 0.30557142\n",
      "Iteration 149, loss = 0.30435616\n",
      "Iteration 150, loss = 0.30558756\n",
      "Iteration 151, loss = 0.30375233\n",
      "Iteration 152, loss = 0.30337066\n",
      "Iteration 153, loss = 0.30340025\n",
      "Iteration 154, loss = 0.30325697\n",
      "Iteration 155, loss = 0.30381834\n",
      "Iteration 156, loss = 0.30253552\n",
      "Iteration 157, loss = 0.30263374\n",
      "Iteration 158, loss = 0.30190924\n",
      "Iteration 159, loss = 0.30059374\n",
      "Iteration 160, loss = 0.30189340\n",
      "Iteration 161, loss = 0.30063793\n",
      "Iteration 162, loss = 0.29942556\n",
      "Iteration 163, loss = 0.29974425\n",
      "Iteration 164, loss = 0.30051663\n",
      "Iteration 165, loss = 0.30031484\n",
      "Iteration 166, loss = 0.29896116\n",
      "Iteration 167, loss = 0.29997388\n",
      "Iteration 168, loss = 0.30064641\n",
      "Iteration 169, loss = 0.29955430\n",
      "Iteration 170, loss = 0.29861862\n",
      "Iteration 171, loss = 0.29815518\n",
      "Iteration 172, loss = 0.29963811\n",
      "Iteration 173, loss = 0.29795173\n",
      "Iteration 174, loss = 0.29822245\n",
      "Iteration 175, loss = 0.29729052\n",
      "Iteration 176, loss = 0.29712067\n",
      "Iteration 177, loss = 0.29749364\n",
      "Iteration 178, loss = 0.29586185\n",
      "Iteration 179, loss = 0.29552258\n",
      "Iteration 180, loss = 0.29703770\n",
      "Iteration 181, loss = 0.29564241\n",
      "Iteration 182, loss = 0.29610381\n",
      "Iteration 183, loss = 0.29476764\n",
      "Iteration 184, loss = 0.29609159\n",
      "Iteration 185, loss = 0.29399469\n",
      "Iteration 186, loss = 0.29445322\n",
      "Iteration 187, loss = 0.29551680\n",
      "Iteration 188, loss = 0.29512277\n",
      "Iteration 189, loss = 0.29564413\n",
      "Iteration 190, loss = 0.29348759\n",
      "Iteration 191, loss = 0.29257255\n",
      "Iteration 192, loss = 0.29278154\n",
      "Iteration 193, loss = 0.29356197\n",
      "Iteration 194, loss = 0.29317063\n",
      "Iteration 195, loss = 0.29325913\n",
      "Iteration 196, loss = 0.29337708\n",
      "Iteration 197, loss = 0.29329137\n",
      "Iteration 198, loss = 0.29399428\n",
      "Iteration 199, loss = 0.29208711\n",
      "Iteration 200, loss = 0.29109774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.53752783\n",
      "Iteration 2, loss = 0.50891904\n",
      "Iteration 3, loss = 0.49905955\n",
      "Iteration 4, loss = 0.49149375\n",
      "Iteration 5, loss = 0.48420474\n",
      "Iteration 6, loss = 0.47792962\n",
      "Iteration 7, loss = 0.47183839\n",
      "Iteration 8, loss = 0.46624338\n",
      "Iteration 9, loss = 0.46104856\n",
      "Iteration 10, loss = 0.45555564\n",
      "Iteration 11, loss = 0.45147219\n",
      "Iteration 12, loss = 0.44758592\n",
      "Iteration 13, loss = 0.44388629\n",
      "Iteration 14, loss = 0.44049135\n",
      "Iteration 15, loss = 0.43620419\n",
      "Iteration 16, loss = 0.43335012\n",
      "Iteration 17, loss = 0.43026649\n",
      "Iteration 18, loss = 0.42612197\n",
      "Iteration 19, loss = 0.42479497\n",
      "Iteration 20, loss = 0.42074222\n",
      "Iteration 21, loss = 0.41845695\n",
      "Iteration 22, loss = 0.41628517\n",
      "Iteration 23, loss = 0.41363509\n",
      "Iteration 24, loss = 0.41073880\n",
      "Iteration 25, loss = 0.40831897\n",
      "Iteration 26, loss = 0.40719827\n",
      "Iteration 27, loss = 0.40422304\n",
      "Iteration 28, loss = 0.40244018\n",
      "Iteration 29, loss = 0.40030742\n",
      "Iteration 30, loss = 0.39872436\n",
      "Iteration 31, loss = 0.39674003\n",
      "Iteration 32, loss = 0.39494820\n",
      "Iteration 33, loss = 0.39300214\n",
      "Iteration 34, loss = 0.39082843\n",
      "Iteration 35, loss = 0.38962161\n",
      "Iteration 36, loss = 0.38787263\n",
      "Iteration 37, loss = 0.38690273\n",
      "Iteration 38, loss = 0.38607456\n",
      "Iteration 39, loss = 0.38418582\n",
      "Iteration 40, loss = 0.38206050\n",
      "Iteration 41, loss = 0.38093618\n",
      "Iteration 42, loss = 0.38073573\n",
      "Iteration 43, loss = 0.37785820\n",
      "Iteration 44, loss = 0.37745746\n",
      "Iteration 45, loss = 0.37701380\n",
      "Iteration 46, loss = 0.37493941\n",
      "Iteration 47, loss = 0.37432350\n",
      "Iteration 48, loss = 0.37357753\n",
      "Iteration 49, loss = 0.37127030\n",
      "Iteration 50, loss = 0.37179677\n",
      "Iteration 51, loss = 0.36945773\n",
      "Iteration 52, loss = 0.36916628\n",
      "Iteration 53, loss = 0.36784630\n",
      "Iteration 54, loss = 0.36825631\n",
      "Iteration 55, loss = 0.36621576\n",
      "Iteration 56, loss = 0.36564394\n",
      "Iteration 57, loss = 0.36379575\n",
      "Iteration 58, loss = 0.36360529\n",
      "Iteration 59, loss = 0.36266572\n",
      "Iteration 60, loss = 0.36144668\n",
      "Iteration 61, loss = 0.36153228\n",
      "Iteration 62, loss = 0.36023169\n",
      "Iteration 63, loss = 0.35889606\n",
      "Iteration 64, loss = 0.35887314\n",
      "Iteration 65, loss = 0.35790236\n",
      "Iteration 66, loss = 0.35751950\n",
      "Iteration 67, loss = 0.35612898\n",
      "Iteration 68, loss = 0.35529201\n",
      "Iteration 69, loss = 0.35484176\n",
      "Iteration 70, loss = 0.35288600\n",
      "Iteration 71, loss = 0.35214679\n",
      "Iteration 72, loss = 0.35322669\n",
      "Iteration 73, loss = 0.35184906\n",
      "Iteration 74, loss = 0.35144414\n",
      "Iteration 75, loss = 0.35180917\n",
      "Iteration 76, loss = 0.35003310\n",
      "Iteration 77, loss = 0.35061743\n",
      "Iteration 78, loss = 0.34821653\n",
      "Iteration 79, loss = 0.34934226\n",
      "Iteration 80, loss = 0.34820378\n",
      "Iteration 81, loss = 0.34737943\n",
      "Iteration 82, loss = 0.34633445\n",
      "Iteration 83, loss = 0.34570164\n",
      "Iteration 84, loss = 0.34446426\n",
      "Iteration 85, loss = 0.34468500\n",
      "Iteration 86, loss = 0.34669357\n",
      "Iteration 87, loss = 0.34319131\n",
      "Iteration 88, loss = 0.34336329\n",
      "Iteration 89, loss = 0.34027787\n",
      "Iteration 90, loss = 0.34305405\n",
      "Iteration 91, loss = 0.34127202\n",
      "Iteration 92, loss = 0.34205283\n",
      "Iteration 93, loss = 0.34092291\n",
      "Iteration 94, loss = 0.33983567\n",
      "Iteration 95, loss = 0.34182288\n",
      "Iteration 96, loss = 0.33977621\n",
      "Iteration 97, loss = 0.33940424\n",
      "Iteration 98, loss = 0.33909656\n",
      "Iteration 99, loss = 0.33848803\n",
      "Iteration 100, loss = 0.33855618\n",
      "Iteration 101, loss = 0.33579786\n",
      "Iteration 102, loss = 0.33599127\n",
      "Iteration 103, loss = 0.33750337\n",
      "Iteration 104, loss = 0.33701180\n",
      "Iteration 105, loss = 0.33587356\n",
      "Iteration 106, loss = 0.33517580\n",
      "Iteration 107, loss = 0.33540167\n",
      "Iteration 108, loss = 0.33395864\n",
      "Iteration 109, loss = 0.33390345\n",
      "Iteration 110, loss = 0.33472622\n",
      "Iteration 111, loss = 0.33580279\n",
      "Iteration 112, loss = 0.33314836\n",
      "Iteration 113, loss = 0.33281008\n",
      "Iteration 114, loss = 0.33321518\n",
      "Iteration 115, loss = 0.33225578\n",
      "Iteration 116, loss = 0.33267539\n",
      "Iteration 117, loss = 0.33096209\n",
      "Iteration 118, loss = 0.33148391\n",
      "Iteration 119, loss = 0.33078978\n",
      "Iteration 120, loss = 0.33030438\n",
      "Iteration 121, loss = 0.33001079\n",
      "Iteration 122, loss = 0.32976059\n",
      "Iteration 123, loss = 0.32920478\n",
      "Iteration 124, loss = 0.32877187\n",
      "Iteration 125, loss = 0.32828972\n",
      "Iteration 126, loss = 0.32787594\n",
      "Iteration 127, loss = 0.32941755\n",
      "Iteration 128, loss = 0.32892637\n",
      "Iteration 129, loss = 0.32906603\n",
      "Iteration 130, loss = 0.32749045\n",
      "Iteration 131, loss = 0.32673395\n",
      "Iteration 132, loss = 0.32726407\n",
      "Iteration 133, loss = 0.32656569\n",
      "Iteration 134, loss = 0.32636917\n",
      "Iteration 135, loss = 0.32592277\n",
      "Iteration 136, loss = 0.32535883\n",
      "Iteration 137, loss = 0.32544526\n",
      "Iteration 138, loss = 0.32620900\n",
      "Iteration 139, loss = 0.32411301\n",
      "Iteration 140, loss = 0.32488038\n",
      "Iteration 141, loss = 0.32467824\n",
      "Iteration 142, loss = 0.32481506\n",
      "Iteration 143, loss = 0.32344796\n",
      "Iteration 144, loss = 0.32478031\n",
      "Iteration 145, loss = 0.32307445\n",
      "Iteration 146, loss = 0.32258128\n",
      "Iteration 147, loss = 0.32403210\n",
      "Iteration 148, loss = 0.32311485\n",
      "Iteration 149, loss = 0.32190119\n",
      "Iteration 150, loss = 0.32243505\n",
      "Iteration 151, loss = 0.32310698\n",
      "Iteration 152, loss = 0.32213646\n",
      "Iteration 153, loss = 0.32244171\n",
      "Iteration 154, loss = 0.32183208\n",
      "Iteration 155, loss = 0.32263247\n",
      "Iteration 156, loss = 0.32134732\n",
      "Iteration 157, loss = 0.32050833\n",
      "Iteration 158, loss = 0.32071230\n",
      "Iteration 159, loss = 0.32150033\n",
      "Iteration 160, loss = 0.31965645\n",
      "Iteration 161, loss = 0.31995946\n",
      "Iteration 162, loss = 0.32001218\n",
      "Iteration 163, loss = 0.31950065\n",
      "Iteration 164, loss = 0.31965317\n",
      "Iteration 165, loss = 0.31915288\n",
      "Iteration 166, loss = 0.31876965\n",
      "Iteration 167, loss = 0.31883000\n",
      "Iteration 168, loss = 0.31851816\n",
      "Iteration 169, loss = 0.31921358\n",
      "Iteration 170, loss = 0.31642218\n",
      "Iteration 171, loss = 0.31766605\n",
      "Iteration 172, loss = 0.31866521\n",
      "Iteration 173, loss = 0.31662524\n",
      "Iteration 174, loss = 0.31725400\n",
      "Iteration 175, loss = 0.31771550\n",
      "Iteration 176, loss = 0.31701767\n",
      "Iteration 177, loss = 0.31637904\n",
      "Iteration 178, loss = 0.31619281\n",
      "Iteration 179, loss = 0.31613927\n",
      "Iteration 180, loss = 0.31577527\n",
      "Iteration 181, loss = 0.31655307\n",
      "Iteration 182, loss = 0.31460660\n",
      "Iteration 183, loss = 0.31522873\n",
      "Iteration 184, loss = 0.31620204\n",
      "Iteration 185, loss = 0.31456369\n",
      "Iteration 186, loss = 0.31429004\n",
      "Iteration 187, loss = 0.31540452\n",
      "Iteration 188, loss = 0.31443464\n",
      "Iteration 189, loss = 0.31529138\n",
      "Iteration 190, loss = 0.31408254\n",
      "Iteration 191, loss = 0.31371247\n",
      "Iteration 192, loss = 0.31548312\n",
      "Iteration 193, loss = 0.31579104\n",
      "Iteration 194, loss = 0.31247395\n",
      "Iteration 195, loss = 0.31367875\n",
      "Iteration 196, loss = 0.31446966\n",
      "Iteration 197, loss = 0.31409951\n",
      "Iteration 198, loss = 0.31447047\n",
      "Iteration 199, loss = 0.31239507\n",
      "Iteration 200, loss = 0.31219327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the classification on GradientBoostingClassifier\n",
      "Running the classification on BaggingClassifier\n",
      "Running the classification on ExtraTreesClassifier\n",
      "Running the classification on DecisionTreeClassifier\n",
      "Running the classification on SVC\n"
     ]
    }
   ],
   "source": [
    "#split dataset in cross-validation with this splitter class: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit\n",
    "#note: this is an alternative to train_test_split\n",
    "cv_split = ShuffleSplit(n_splits = 10, test_size = .20, train_size = .75, \\\n",
    "                                                random_state = 0 )\n",
    "                    # run model 10x with 60/30 split intentionally leaving out 10%\n",
    "\n",
    "#create table to compare MLA metrics\n",
    "MLA_columns = ['MLA Name', 'MLA Parameters','MLA Train Accuracy Mean', 'MLA Test Accuracy Mean', \\\n",
    "               'MLA Test Accuracy 3*STD' ,'MLA Time']\n",
    "MLA_compare = pd.DataFrame(columns = MLA_columns)\n",
    "\n",
    "#create table to compare MLA predictions\n",
    "MLA_predict = Y_all.copy()\n",
    "\n",
    "#index through MLA and save performance to table\n",
    "row_index = 0\n",
    "for alg in MLA:\n",
    "\n",
    "    #set name and parameters\n",
    "    MLA_name = alg.__class__.__name__\n",
    "    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n",
    "    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n",
    "    \n",
    "    print(\"Running the classification on %s\" %(MLA_name))\n",
    "    \n",
    "    #score model with cross validation: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate\n",
    "    cv_results = cross_validate(alg, X_all, Y_all, cv  = cv_split,return_train_score=True)\n",
    "\n",
    "    MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n",
    "    MLA_compare.loc[row_index, 'MLA Train Accuracy Mean'] = cv_results['train_score'].mean()\n",
    "    MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean()   \n",
    "    #if this is a non-bias random sample, then +/-3 standard deviations (std) from the mean, \n",
    "    #should statistically capture 99.7% of the subsets\n",
    "    MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std()*3   \n",
    "    #let's know the worst that can happen!\n",
    "    \n",
    "\n",
    "    #save MLA predictions - see section 6 for usage\n",
    "    alg.fit(X_all, Y_all)\n",
    "    MLA_predict[MLA_name] = alg.predict(X_all)\n",
    "    \n",
    "    row_index+=1\n",
    "\n",
    "    \n",
    "#print and sort table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort_values.html\n",
    "MLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace = True)\n",
    "MLA_compare\n",
    "#MLA_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'KNeighborsClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-c0a086ffbf3d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdegree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# model = RandomForestClassifier(n_estimators = 100,verbose=1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKNeighborsClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_neighbors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m# model = MLPClassifier(verbose=1,max_iter=300,hidden_layer_sizes=(150,100,10,2,))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtrain_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdegree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'KNeighborsClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "degree = 1\n",
    "# model = RandomForestClassifier(n_estimators = 100,verbose=1)\n",
    "# model = KNeighborsClassifier(n_neighbors = 3)\n",
    "# model = MLPClassifier(verbose=1,max_iter=300,hidden_layer_sizes=(150,100,10,2,))\n",
    "model = SVC()\n",
    "train_test(model,X_train,Y_train,X_test,Y_test,degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Aug_ct</td>\n",
       "      <td>0.919642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>Flashbang_ct</td>\n",
       "      <td>0.246007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>DecoyGrenade_t</td>\n",
       "      <td>0.199311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>kwct_P250</td>\n",
       "      <td>0.193878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>defuse_kit_ct1</td>\n",
       "      <td>0.191431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>DecoyGrenade_ct</td>\n",
       "      <td>0.174686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>has_helmet_t1</td>\n",
       "      <td>0.159686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>alive_players_t</td>\n",
       "      <td>0.154433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Deagle_ct</td>\n",
       "      <td>0.153462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>alive_players_ct</td>\n",
       "      <td>0.148785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>kwct_Ak47</td>\n",
       "      <td>0.134403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>armor_ct3_Bin_Code</td>\n",
       "      <td>0.125695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>armor_ct1_Bin_Code</td>\n",
       "      <td>0.112414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>P2000_t</td>\n",
       "      <td>0.087896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>Deagle_t</td>\n",
       "      <td>0.079995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>armor_ct2_Bin_Code</td>\n",
       "      <td>0.076628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>kwt_Awp</td>\n",
       "      <td>0.072545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>defuse_kit_ct3</td>\n",
       "      <td>0.067566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>P2000_ct</td>\n",
       "      <td>0.064212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>kwct_other_heavy</td>\n",
       "      <td>0.059469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>kwct_MolotovIncendiaryGrenade</td>\n",
       "      <td>0.056351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>armor_ct5_Bin_Code</td>\n",
       "      <td>0.052945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>defuse_kit_ct2</td>\n",
       "      <td>0.052291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Ak47_ct</td>\n",
       "      <td>0.043590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>kwt_Ak47</td>\n",
       "      <td>0.042462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>P250_ct</td>\n",
       "      <td>0.040646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>kwct_Cz75Auto</td>\n",
       "      <td>0.036686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Cz75Auto_ct</td>\n",
       "      <td>0.035495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>Glock_ct</td>\n",
       "      <td>0.034298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>armor_t3_Bin_Code</td>\n",
       "      <td>0.033624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>has_helmet_t5</td>\n",
       "      <td>0.033161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>has_helmet_t3</td>\n",
       "      <td>0.031522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>kwct_Flashbang</td>\n",
       "      <td>0.028303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>kwct_Deagle</td>\n",
       "      <td>0.028219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>P250_t</td>\n",
       "      <td>0.027042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>health_t2_Bin_Code</td>\n",
       "      <td>0.025174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>health_ct3_Bin_Code</td>\n",
       "      <td>0.025144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>Mp9_ct</td>\n",
       "      <td>0.023063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>kwct_Awp</td>\n",
       "      <td>0.022604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>kwct_HeGrenade</td>\n",
       "      <td>0.021850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>kwt_Aug</td>\n",
       "      <td>0.012329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>health_t5_Bin_Code</td>\n",
       "      <td>0.006727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>kwct_M4a4</td>\n",
       "      <td>0.006603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Awp_ct</td>\n",
       "      <td>0.006345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>health_ct4_Bin_Code</td>\n",
       "      <td>0.005121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>kwct_other_utils</td>\n",
       "      <td>0.005088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>kwct_Knife</td>\n",
       "      <td>0.004852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>kwct_other_smgs</td>\n",
       "      <td>0.004284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>health_ct2_Bin_Code</td>\n",
       "      <td>0.004056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>current_score_ct</td>\n",
       "      <td>0.001601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>health_t1_Bin_Code</td>\n",
       "      <td>0.000756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>has_helmet_ct3</td>\n",
       "      <td>0.000694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>M4a4_ct</td>\n",
       "      <td>0.000268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>kwct_Mp9</td>\n",
       "      <td>0.000040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>current_score_t</td>\n",
       "      <td>-0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>Glock_t</td>\n",
       "      <td>-0.001400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>kwct_other_pistols</td>\n",
       "      <td>-0.002305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>health_t3_Bin_Code</td>\n",
       "      <td>-0.004427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>health_ct5_Bin_Code</td>\n",
       "      <td>-0.004779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>C4_t</td>\n",
       "      <td>-0.011927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>armor_t4_Bin_Code</td>\n",
       "      <td>-0.012346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>Mp9_t</td>\n",
       "      <td>-0.013518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>kwct_other_rifles</td>\n",
       "      <td>-0.013739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>kwct_Sg553</td>\n",
       "      <td>-0.018417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>MolotovIncendiaryGrenade_ct</td>\n",
       "      <td>-0.019291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Aug_t</td>\n",
       "      <td>-0.023136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>HeGrenade_ct</td>\n",
       "      <td>-0.023840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>Sg553_ct</td>\n",
       "      <td>-0.024385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>has_helmet_ct4</td>\n",
       "      <td>-0.025253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>defuse_kit_ct4</td>\n",
       "      <td>-0.026090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>kwct_Glock</td>\n",
       "      <td>-0.026535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>kwct_Aug</td>\n",
       "      <td>-0.026690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>armor_t5_Bin_Code</td>\n",
       "      <td>-0.029495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>has_helmet_ct1</td>\n",
       "      <td>-0.031563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>armor_t_Bin_Code</td>\n",
       "      <td>-0.031910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>MolotovIncendiaryGrenade_t</td>\n",
       "      <td>-0.032784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>M4a4_t</td>\n",
       "      <td>-0.035043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>has_helmet_ct2</td>\n",
       "      <td>-0.036688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>armor_ct_Bin_Code</td>\n",
       "      <td>-0.037663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Awp_t</td>\n",
       "      <td>-0.038592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>defuse_kit_ct5</td>\n",
       "      <td>-0.045023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>has_helmet_ct5</td>\n",
       "      <td>-0.046382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>armor_t1_Bin_Code</td>\n",
       "      <td>-0.048791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>armor_t2_Bin_Code</td>\n",
       "      <td>-0.049454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>HeGrenade_t</td>\n",
       "      <td>-0.058276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>kwct_UspS</td>\n",
       "      <td>-0.059725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>kwct_SmokeGrenade</td>\n",
       "      <td>-0.063854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>kwct_P2000</td>\n",
       "      <td>-0.082097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>has_helmet_t2</td>\n",
       "      <td>-0.083151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>kwct_other_world</td>\n",
       "      <td>-0.087233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>health_ct1_Bin_Code</td>\n",
       "      <td>-0.087383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>health_t4_Bin_Code</td>\n",
       "      <td>-0.095840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>armor_ct4_Bin_Code</td>\n",
       "      <td>-0.100896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>has_helmet_t4</td>\n",
       "      <td>-0.106878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Cz75Auto_t</td>\n",
       "      <td>-0.192130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>UspS_ct</td>\n",
       "      <td>-0.199453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>SmokeGrenade_t</td>\n",
       "      <td>-0.204282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>Sg553_t</td>\n",
       "      <td>-0.222182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>UspS_t</td>\n",
       "      <td>-0.246587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>kwct_C4</td>\n",
       "      <td>-0.247044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>SmokeGrenade_ct</td>\n",
       "      <td>-0.251919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>Flashbang_t</td>\n",
       "      <td>-0.433393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Ak47_t</td>\n",
       "      <td>-0.636501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>kwt_C4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>kwt_Cz75Auto</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>kwt_Deagle</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>kwt_Flashbang</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>kwt_Glock</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>kwt_HeGrenade</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>kwt_Knife</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>kwt_M4a4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>kwt_MolotovIncendiaryGrenade</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>kwt_Mp9</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>kwt_P2000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>kwt_P250</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>kwt_Sg553</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>kwt_SmokeGrenade</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>kwt_UspS</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>kwt_other_heavy</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>kwt_other_pistols</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>kwt_other_rifles</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>kwt_other_smgs</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>kwt_other_utils</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>kwt_other_world</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>map_de_cache</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>map_de_dust2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>map_de_inferno</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>map_de_mirage</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>map_de_nuke</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>map_de_overpass</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>map_de_train</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>map_de_vertigo</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>money_ct1_Bin_Code</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>money_ct2_Bin_Code</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>money_ct3_Bin_Code</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>money_ct4_Bin_Code</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>money_ct5_Bin_Code</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>money_ct_Bin_Code</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>money_t1_Bin_Code</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>money_t2_Bin_Code</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>money_t3_Bin_Code</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>money_t4_Bin_Code</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>money_t5_Bin_Code</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>money_t_Bin_Code</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>other_heavy_ct</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>other_heavy_t</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>other_pistols_ct</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>other_pistols_t</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>other_rifles_ct</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>other_rifles_t</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>other_smgs_ct</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>other_smgs_t</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>pos_bs_ct1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>pos_bs_ct2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>pos_bs_ct3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>pos_bs_ct4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>pos_bs_ct5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>pos_bs_t1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>pos_bs_t2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>pos_bs_t3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>pos_bs_t4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>pos_bs_t5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>pr_ct1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>pr_ct2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>pr_ct3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>pr_ct4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>pr_ct5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>pr_t1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>pr_t2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>pr_t3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>pr_t4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>pr_t5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>round_status_BombPlanted</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>round_status_FreezeTime</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>round_status_Normal</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>round_status_time_left</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>round_winner_t</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>t_leads</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Feature  Correlation\n",
       "2                           Aug_ct     0.919642\n",
       "13                    Flashbang_ct     0.246007\n",
       "12                  DecoyGrenade_t     0.199311\n",
       "90                       kwct_P250     0.193878\n",
       "51                  defuse_kit_ct1     0.191431\n",
       "11                 DecoyGrenade_ct     0.174686\n",
       "61                   has_helmet_t1     0.159686\n",
       "36                 alive_players_t     0.154433\n",
       "9                        Deagle_ct     0.153462\n",
       "35                alive_players_ct     0.148785\n",
       "76                       kwct_Ak47     0.134403\n",
       "39              armor_ct3_Bin_Code     0.125695\n",
       "37              armor_ct1_Bin_Code     0.112414\n",
       "26                         P2000_t     0.087896\n",
       "10                        Deagle_t     0.079995\n",
       "38              armor_ct2_Bin_Code     0.076628\n",
       "102                        kwt_Awp     0.072545\n",
       "53                  defuse_kit_ct3     0.067566\n",
       "25                        P2000_ct     0.064212\n",
       "94                kwct_other_heavy     0.059469\n",
       "87   kwct_MolotovIncendiaryGrenade     0.056351\n",
       "41              armor_ct5_Bin_Code     0.052945\n",
       "52                  defuse_kit_ct2     0.052291\n",
       "0                          Ak47_ct     0.043590\n",
       "100                       kwt_Ak47     0.042462\n",
       "27                         P250_ct     0.040646\n",
       "80                   kwct_Cz75Auto     0.036686\n",
       "7                      Cz75Auto_ct     0.035495\n",
       "15                        Glock_ct     0.034298\n",
       "45               armor_t3_Bin_Code     0.033624\n",
       "65                   has_helmet_t5     0.033161\n",
       "63                   has_helmet_t3     0.031522\n",
       "82                  kwct_Flashbang     0.028303\n",
       "81                     kwct_Deagle     0.028219\n",
       "28                          P250_t     0.027042\n",
       "72              health_t2_Bin_Code     0.025174\n",
       "68             health_ct3_Bin_Code     0.025144\n",
       "23                          Mp9_ct     0.023063\n",
       "78                        kwct_Awp     0.022604\n",
       "84                  kwct_HeGrenade     0.021850\n",
       "101                        kwt_Aug     0.012329\n",
       "75              health_t5_Bin_Code     0.006727\n",
       "86                       kwct_M4a4     0.006603\n",
       "4                           Awp_ct     0.006345\n",
       "69             health_ct4_Bin_Code     0.005121\n",
       "98                kwct_other_utils     0.005088\n",
       "85                      kwct_Knife     0.004852\n",
       "97                 kwct_other_smgs     0.004284\n",
       "67             health_ct2_Bin_Code     0.004056\n",
       "49                current_score_ct     0.001601\n",
       "71              health_t1_Bin_Code     0.000756\n",
       "58                  has_helmet_ct3     0.000694\n",
       "19                         M4a4_ct     0.000268\n",
       "88                        kwct_Mp9     0.000040\n",
       "50                 current_score_t    -0.000001\n",
       "16                         Glock_t    -0.001400\n",
       "95              kwct_other_pistols    -0.002305\n",
       "73              health_t3_Bin_Code    -0.004427\n",
       "70             health_ct5_Bin_Code    -0.004779\n",
       "6                             C4_t    -0.011927\n",
       "46               armor_t4_Bin_Code    -0.012346\n",
       "24                           Mp9_t    -0.013518\n",
       "96               kwct_other_rifles    -0.013739\n",
       "91                      kwct_Sg553    -0.018417\n",
       "21     MolotovIncendiaryGrenade_ct    -0.019291\n",
       "3                            Aug_t    -0.023136\n",
       "17                    HeGrenade_ct    -0.023840\n",
       "29                        Sg553_ct    -0.024385\n",
       "59                  has_helmet_ct4    -0.025253\n",
       "54                  defuse_kit_ct4    -0.026090\n",
       "83                      kwct_Glock    -0.026535\n",
       "77                        kwct_Aug    -0.026690\n",
       "47               armor_t5_Bin_Code    -0.029495\n",
       "56                  has_helmet_ct1    -0.031563\n",
       "48                armor_t_Bin_Code    -0.031910\n",
       "22      MolotovIncendiaryGrenade_t    -0.032784\n",
       "20                          M4a4_t    -0.035043\n",
       "57                  has_helmet_ct2    -0.036688\n",
       "42               armor_ct_Bin_Code    -0.037663\n",
       "5                            Awp_t    -0.038592\n",
       "55                  defuse_kit_ct5    -0.045023\n",
       "60                  has_helmet_ct5    -0.046382\n",
       "43               armor_t1_Bin_Code    -0.048791\n",
       "44               armor_t2_Bin_Code    -0.049454\n",
       "18                     HeGrenade_t    -0.058276\n",
       "93                       kwct_UspS    -0.059725\n",
       "92               kwct_SmokeGrenade    -0.063854\n",
       "89                      kwct_P2000    -0.082097\n",
       "62                   has_helmet_t2    -0.083151\n",
       "99                kwct_other_world    -0.087233\n",
       "66             health_ct1_Bin_Code    -0.087383\n",
       "74              health_t4_Bin_Code    -0.095840\n",
       "40              armor_ct4_Bin_Code    -0.100896\n",
       "64                   has_helmet_t4    -0.106878\n",
       "8                       Cz75Auto_t    -0.192130\n",
       "33                         UspS_ct    -0.199453\n",
       "32                  SmokeGrenade_t    -0.204282\n",
       "30                         Sg553_t    -0.222182\n",
       "34                          UspS_t    -0.246587\n",
       "79                         kwct_C4    -0.247044\n",
       "31                 SmokeGrenade_ct    -0.251919\n",
       "14                     Flashbang_t    -0.433393\n",
       "1                           Ak47_t    -0.636501\n",
       "103                         kwt_C4          NaN\n",
       "104                   kwt_Cz75Auto          NaN\n",
       "105                     kwt_Deagle          NaN\n",
       "106                  kwt_Flashbang          NaN\n",
       "107                      kwt_Glock          NaN\n",
       "108                  kwt_HeGrenade          NaN\n",
       "109                      kwt_Knife          NaN\n",
       "110                       kwt_M4a4          NaN\n",
       "111   kwt_MolotovIncendiaryGrenade          NaN\n",
       "112                        kwt_Mp9          NaN\n",
       "113                      kwt_P2000          NaN\n",
       "114                       kwt_P250          NaN\n",
       "115                      kwt_Sg553          NaN\n",
       "116               kwt_SmokeGrenade          NaN\n",
       "117                       kwt_UspS          NaN\n",
       "118                kwt_other_heavy          NaN\n",
       "119              kwt_other_pistols          NaN\n",
       "120               kwt_other_rifles          NaN\n",
       "121                 kwt_other_smgs          NaN\n",
       "122                kwt_other_utils          NaN\n",
       "123                kwt_other_world          NaN\n",
       "124                   map_de_cache          NaN\n",
       "125                   map_de_dust2          NaN\n",
       "126                 map_de_inferno          NaN\n",
       "127                  map_de_mirage          NaN\n",
       "128                    map_de_nuke          NaN\n",
       "129                map_de_overpass          NaN\n",
       "130                   map_de_train          NaN\n",
       "131                 map_de_vertigo          NaN\n",
       "132             money_ct1_Bin_Code          NaN\n",
       "133             money_ct2_Bin_Code          NaN\n",
       "134             money_ct3_Bin_Code          NaN\n",
       "135             money_ct4_Bin_Code          NaN\n",
       "136             money_ct5_Bin_Code          NaN\n",
       "137              money_ct_Bin_Code          NaN\n",
       "138              money_t1_Bin_Code          NaN\n",
       "139              money_t2_Bin_Code          NaN\n",
       "140              money_t3_Bin_Code          NaN\n",
       "141              money_t4_Bin_Code          NaN\n",
       "142              money_t5_Bin_Code          NaN\n",
       "143               money_t_Bin_Code          NaN\n",
       "144                 other_heavy_ct          NaN\n",
       "145                  other_heavy_t          NaN\n",
       "146               other_pistols_ct          NaN\n",
       "147                other_pistols_t          NaN\n",
       "148                other_rifles_ct          NaN\n",
       "149                 other_rifles_t          NaN\n",
       "150                  other_smgs_ct          NaN\n",
       "151                   other_smgs_t          NaN\n",
       "152                     pos_bs_ct1          NaN\n",
       "153                     pos_bs_ct2          NaN\n",
       "154                     pos_bs_ct3          NaN\n",
       "155                     pos_bs_ct4          NaN\n",
       "156                     pos_bs_ct5          NaN\n",
       "157                      pos_bs_t1          NaN\n",
       "158                      pos_bs_t2          NaN\n",
       "159                      pos_bs_t3          NaN\n",
       "160                      pos_bs_t4          NaN\n",
       "161                      pos_bs_t5          NaN\n",
       "162                         pr_ct1          NaN\n",
       "163                         pr_ct2          NaN\n",
       "164                         pr_ct3          NaN\n",
       "165                         pr_ct4          NaN\n",
       "166                         pr_ct5          NaN\n",
       "167                          pr_t1          NaN\n",
       "168                          pr_t2          NaN\n",
       "169                          pr_t3          NaN\n",
       "170                          pr_t4          NaN\n",
       "171                          pr_t5          NaN\n",
       "172       round_status_BombPlanted          NaN\n",
       "173        round_status_FreezeTime          NaN\n",
       "174            round_status_Normal          NaN\n",
       "175         round_status_time_left          NaN\n",
       "176                 round_winner_t          NaN\n",
       "177                        t_leads          NaN"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "X_coeff = pd.DataFrame(df.columns)\n",
    "X_coeff.columns = ['Feature']\n",
    "X_coeff[\"Correlation\"] = pd.Series(model.coef_[0])\n",
    "\n",
    "X_coeff.sort_values(by='Correlation', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE:\n",
    "#1. proximity features not working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomForestClassifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
