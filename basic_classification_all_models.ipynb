{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "#models:\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,\\\n",
    "GradientBoostingClassifier, VotingClassifier, BaggingClassifier, ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "##\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import cross_validate\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read all the datasets\n",
    "def read_all(template,start,end):\n",
    "    frames = [ pd.read_json(f).fillna(0) for f in [template.format(i) for i in range(start,end)] ]\n",
    "    X = pd.concat(frames, ignore_index = True,sort = True)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"datasets/dataset_finalized/dataset_{:02}.json\"\n",
    "df = read_all(template,0,25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 122074 entries, 0 to 122073\n",
      "Columns: 185 entries, Ak47_ct to t_leads\n",
      "dtypes: float64(11), int64(174)\n",
      "memory usage: 172.3 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "colwep = ['Ak47_ct', 'Ak47_t', 'Aug_ct', 'Aug_t', 'Awp_ct', 'Awp_t', 'C4_t', 'Cz75Auto_ct',\\\n",
    "          'Cz75Auto_t', 'Deagle_ct', 'Deagle_t', 'DecoyGrenade_ct', 'DecoyGrenade_t', 'Flashbang_ct',\\\n",
    "          'Flashbang_t', 'Glock_ct', 'Glock_t', 'HeGrenade_ct', 'HeGrenade_t', 'M4a4_ct', 'M4a4_t',\\\n",
    "          'MolotovIncendiaryGrenade_ct', 'MolotovIncendiaryGrenade_t', 'Mp9_ct', 'Mp9_t', 'P2000_ct',\\\n",
    "          'P2000_t', 'P250_ct', 'P250_t', 'Sg553_ct', 'Sg553_t', 'SmokeGrenade_ct', 'SmokeGrenade_t',\\\n",
    "          'UspS_ct', 'UspS_t','other_heavy_ct', 'other_heavy_t', 'other_pistols_ct', \\\n",
    "          'other_pistols_t', 'other_rifles_ct', 'other_rifles_t', 'other_smgs_ct', 'other_smgs_t']\n",
    "\n",
    "# colwep = ['Ak47_ct', 'Ak47_t', 'Aug_ct', 'Aug_t', 'Awp_ct', 'Awp_t', 'C4_t',\\\n",
    "#           'Deagle_ct', 'Deagle_t', 'Flashbang_ct',\\\n",
    "#           'Flashbang_t', 'Glock_ct', 'Glock_t', 'HeGrenade_ct', 'HeGrenade_t', 'M4a4_ct', 'M4a4_t',\\\n",
    "#           'MolotovIncendiaryGrenade_ct', 'MolotovIncendiaryGrenade_t', 'P2000_ct',\\\n",
    "#           'P2000_t', 'P250_ct', 'P250_t', 'Sg553_ct', 'Sg553_t', 'SmokeGrenade_ct', 'SmokeGrenade_t',\\\n",
    "#           'UspS_ct', 'UspS_t']\n",
    "\n",
    "colpla = ['alive_players_ct', 'alive_players_t', 'armor_ct1_Bin_Code', 'armor_ct2_Bin_Code',\\\n",
    "          'armor_ct3_Bin_Code', 'armor_ct4_Bin_Code', 'armor_ct5_Bin_Code', 'armor_ct_Bin_Code',\\\n",
    "          'armor_t1_Bin_Code', 'armor_t2_Bin_Code', 'armor_t3_Bin_Code', 'armor_t4_Bin_Code',\\\n",
    "          'armor_t5_Bin_Code', 'armor_t_Bin_Code','defuse_kit_ct1', 'defuse_kit_ct2',\\\n",
    "          'defuse_kit_ct3', 'defuse_kit_ct4', 'defuse_kit_ct5','defuse_kit_ct', \\\n",
    "          'has_helmet_ct1', 'has_helmet_ct2', 'has_helmet_ct3', 'has_helmet_ct4', 'has_helmet_ct5', \\\n",
    "          'has_helmet_ct','has_helmet_t1', 'has_helmet_t2', 'has_helmet_t3', 'has_helmet_t4',\\\n",
    "          'has_helmet_t5','has_helmet_t', 'health_ct1_Bin_Code', 'health_ct2_Bin_Code', \\\n",
    "          'health_ct3_Bin_Code', 'health_ct4_Bin_Code', 'health_ct5_Bin_Code','health_ct_Bin_Code',\\\n",
    "          'health_t1_Bin_Code', 'health_t2_Bin_Code', 'health_t3_Bin_Code', 'health_t4_Bin_Code',\\\n",
    "          'health_t5_Bin_Code','health_t_Bin_Code','money_ct1_Bin_Code', 'money_ct2_Bin_Code',\\\n",
    "          'money_ct3_Bin_Code', 'money_ct4_Bin_Code', 'money_ct5_Bin_Code', \\\n",
    "          'money_ct_Bin_Code', 'money_t1_Bin_Code', 'money_t2_Bin_Code', 'money_t3_Bin_Code', \\\n",
    "          'money_t4_Bin_Code', 'money_t5_Bin_Code', 'money_t_Bin_Code']\n",
    "\n",
    "colsta = ['current_score_ct', 'current_score_t','t_leads','round_status_BombPlanted',\\\n",
    "          'round_status_FreezeTime', 'round_status_Normal', 'round_status_time_left']\n",
    "\n",
    "colkill = ['kwct_Ak47', 'kwct_Aug', 'kwct_Awp', 'kwct_C4', 'kwct_Cz75Auto', 'kwct_Deagle',\\\n",
    "           'kwct_Flashbang', 'kwct_Glock', 'kwct_HeGrenade', 'kwct_Knife', 'kwct_M4a4',\\\n",
    "           'kwct_MolotovIncendiaryGrenade', 'kwct_Mp9', 'kwct_P2000', 'kwct_P250', 'kwct_Sg553',\\\n",
    "           'kwct_SmokeGrenade', 'kwct_UspS', 'kwct_other_heavy', 'kwct_other_pistols',\\\n",
    "           'kwct_other_rifles', 'kwct_other_smgs', 'kwct_other_utils', 'kwct_other_world', 'kwt_Ak47',\\\n",
    "           'kwt_Aug', 'kwt_Awp', 'kwt_C4', 'kwt_Cz75Auto', 'kwt_Deagle', 'kwt_Flashbang', 'kwt_Glock',\\\n",
    "           'kwt_HeGrenade', 'kwt_Knife', 'kwt_M4a4', 'kwt_MolotovIncendiaryGrenade', 'kwt_Mp9',\\\n",
    "           'kwt_P2000', 'kwt_P250', 'kwt_Sg553', 'kwt_SmokeGrenade', 'kwt_UspS', 'kwt_other_heavy',\\\n",
    "           'kwt_other_pistols', 'kwt_other_rifles', 'kwt_other_smgs', 'kwt_other_utils',\\\n",
    "           'kwt_other_world']\n",
    "\n",
    "colmap = ['map_de_dust2', 'map_de_inferno', 'map_de_mirage', 'map_de_nuke', 'map_de_overpass',\\\n",
    "          'map_de_train', 'map_de_vertigo','map_de_cache']\n",
    "\n",
    "colpos = ['pos_bs_ct1', 'pos_bs_ct2', 'pos_bs_ct3', 'pos_bs_ct4', 'pos_bs_ct5', 'pos_bs_t1',\\\n",
    "          'pos_bs_t2', 'pos_bs_t3', 'pos_bs_t4', 'pos_bs_t5']\n",
    "\n",
    "# colpos = ['pos_bs_ct1', 'pos_bs_ct2', 'pos_bs_ct3', 'pos_bs_ct4', 'pos_bs_ct5', 'pos_bs_t1',\\\n",
    "#           'pos_bs_t2', 'pos_bs_t3', 'pos_bs_t4', 'pos_bs_t5', 'pr_ct1','pr_ct2', 'pr_ct3',\\\n",
    "#           'pr_ct4', 'pr_ct5', 'pr_t1', 'pr_t2', 'pr_t3', 'pr_t4', 'pr_t5']\n",
    "\n",
    "cols = colpla+colmap+colwep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_all = df['round_winner_t']\n",
    "X_all = df.drop(columns='round_winner_t',axis=1)[cols]\n",
    "X_all = StandardScaler().fit_transform(X_all) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(122074, 107)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(122074,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,X_t,Y_t,degree):\n",
    "    pr = PolynomialFeatures(degree=degree,include_bias=True)\n",
    "    X_p = pr.fit_transform(X_t)\n",
    "    model.fit(X_p,Y_t)\n",
    "    pred_train = model.predict(X_p)\n",
    "    \n",
    "    return pred_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model,X_t,degree):\n",
    "    pr = PolynomialFeatures(degree=degree,include_bias=True)\n",
    "    X_p = pr.fit_transform(X_t)\n",
    "    \n",
    "    return model.predict(X_p)\n",
    "\n",
    "def get_mse(model,Y_train,pred_train,Y_test,pred_test):\n",
    "    train_mse = mean_squared_error(Y_train, pred_train)\n",
    "    test_mse = mean_squared_error(Y_test, pred_test)\n",
    "    print(\"Training MSE = {}\".format(train_mse))\n",
    "    print(\"Test MSE = {}\".format(test_mse))\n",
    "\n",
    "def train_test(model,X_train,Y_train,X_test,Y_test,degree):\n",
    "    pred_train = train_model(model,X_train,Y_train,degree)\n",
    "    pred_test = test_model(model,X_test,degree)\n",
    "    get_mse(model,Y_train,pred_train,Y_test,pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_all, Y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLA = [\n",
    "    LinearRegression(),\n",
    "    LogisticRegression(solver='sag',verbose=1),\n",
    "    GaussianNB(),\n",
    "    MLPClassifier(verbose=1),\n",
    "    GradientBoostingClassifier(),\n",
    "# #     VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)]),\n",
    "    BaggingClassifier(),\n",
    "    ExtraTreesClassifier(),\n",
    "    DecisionTreeClassifier(),\n",
    "#     SVC(probability=True),\n",
    "#     KNeighborsClassifier(n_neighbors = 3),\n",
    "#     RandomForestClassifier(n_estimators = 100,verbose=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the classification on LinearRegression\n",
      "Running the classification on LogisticRegression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 53 epochs took 7 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    7.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 44 epochs took 8 seconds\n",
      "Running the classification on GaussianNB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    8.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the classification on MLPClassifier\n",
      "Iteration 1, loss = 0.53739358\n",
      "Iteration 2, loss = 0.51362031\n",
      "Iteration 3, loss = 0.50514391\n",
      "Iteration 4, loss = 0.49814693\n",
      "Iteration 5, loss = 0.49254717\n",
      "Iteration 6, loss = 0.48711161\n",
      "Iteration 7, loss = 0.48236888\n",
      "Iteration 8, loss = 0.47753622\n",
      "Iteration 9, loss = 0.47371450\n",
      "Iteration 10, loss = 0.46887225\n",
      "Iteration 11, loss = 0.46561078\n",
      "Iteration 12, loss = 0.46281177\n",
      "Iteration 13, loss = 0.45796733\n",
      "Iteration 14, loss = 0.45598923\n",
      "Iteration 15, loss = 0.45267358\n",
      "Iteration 16, loss = 0.45012279\n",
      "Iteration 17, loss = 0.44709402\n",
      "Iteration 18, loss = 0.44462043\n",
      "Iteration 19, loss = 0.44299496\n",
      "Iteration 20, loss = 0.44063298\n",
      "Iteration 21, loss = 0.43742140\n",
      "Iteration 22, loss = 0.43637923\n",
      "Iteration 23, loss = 0.43365747\n",
      "Iteration 24, loss = 0.43203854\n",
      "Iteration 25, loss = 0.42984914\n",
      "Iteration 26, loss = 0.42833409\n",
      "Iteration 27, loss = 0.42703462\n",
      "Iteration 28, loss = 0.42500137\n",
      "Iteration 29, loss = 0.42267289\n",
      "Iteration 30, loss = 0.42240401\n",
      "Iteration 31, loss = 0.41991214\n",
      "Iteration 32, loss = 0.41823712\n",
      "Iteration 33, loss = 0.41684861\n",
      "Iteration 34, loss = 0.41561991\n",
      "Iteration 35, loss = 0.41421006\n",
      "Iteration 36, loss = 0.41306257\n",
      "Iteration 37, loss = 0.41152501\n",
      "Iteration 38, loss = 0.41073726\n",
      "Iteration 39, loss = 0.40869750\n",
      "Iteration 40, loss = 0.40795442\n",
      "Iteration 41, loss = 0.40703875\n",
      "Iteration 42, loss = 0.40573710\n",
      "Iteration 43, loss = 0.40445226\n",
      "Iteration 44, loss = 0.40260922\n",
      "Iteration 45, loss = 0.40164040\n",
      "Iteration 46, loss = 0.40218189\n",
      "Iteration 47, loss = 0.40050329\n",
      "Iteration 48, loss = 0.39860420\n",
      "Iteration 49, loss = 0.39899639\n",
      "Iteration 50, loss = 0.39658347\n",
      "Iteration 51, loss = 0.39696240\n",
      "Iteration 52, loss = 0.39635963\n",
      "Iteration 53, loss = 0.39397417\n",
      "Iteration 54, loss = 0.39435391\n",
      "Iteration 55, loss = 0.39265929\n",
      "Iteration 56, loss = 0.39295250\n",
      "Iteration 57, loss = 0.39080217\n",
      "Iteration 58, loss = 0.39028942\n",
      "Iteration 59, loss = 0.38977673\n",
      "Iteration 60, loss = 0.38802662\n",
      "Iteration 61, loss = 0.38744467\n",
      "Iteration 62, loss = 0.38800272\n",
      "Iteration 63, loss = 0.38656818\n",
      "Iteration 64, loss = 0.38532655\n",
      "Iteration 65, loss = 0.38509544\n",
      "Iteration 66, loss = 0.38469417\n",
      "Iteration 67, loss = 0.38462127\n",
      "Iteration 68, loss = 0.38314412\n",
      "Iteration 69, loss = 0.38291050\n",
      "Iteration 70, loss = 0.38187314\n",
      "Iteration 71, loss = 0.38081679\n",
      "Iteration 72, loss = 0.38039026\n",
      "Iteration 73, loss = 0.38013167\n",
      "Iteration 74, loss = 0.37932794\n",
      "Iteration 75, loss = 0.37790048\n",
      "Iteration 76, loss = 0.37872881\n",
      "Iteration 77, loss = 0.37755461\n",
      "Iteration 78, loss = 0.37698523\n",
      "Iteration 79, loss = 0.37667913\n",
      "Iteration 80, loss = 0.37561469\n",
      "Iteration 81, loss = 0.37417862\n",
      "Iteration 82, loss = 0.37434587\n",
      "Iteration 83, loss = 0.37411734\n",
      "Iteration 84, loss = 0.37295193\n",
      "Iteration 85, loss = 0.37246841\n",
      "Iteration 86, loss = 0.37233265\n",
      "Iteration 87, loss = 0.37181370\n",
      "Iteration 88, loss = 0.37080935\n",
      "Iteration 89, loss = 0.37108902\n",
      "Iteration 90, loss = 0.37068830\n",
      "Iteration 91, loss = 0.37015794\n",
      "Iteration 92, loss = 0.37142100\n",
      "Iteration 93, loss = 0.37005185\n",
      "Iteration 94, loss = 0.36848317\n",
      "Iteration 95, loss = 0.36830997\n",
      "Iteration 96, loss = 0.36767278\n",
      "Iteration 97, loss = 0.36855069\n",
      "Iteration 98, loss = 0.36664687\n",
      "Iteration 99, loss = 0.36596018\n",
      "Iteration 100, loss = 0.36762893\n",
      "Iteration 101, loss = 0.36587246\n",
      "Iteration 102, loss = 0.36544128\n",
      "Iteration 103, loss = 0.36533249\n",
      "Iteration 104, loss = 0.36396575\n",
      "Iteration 105, loss = 0.36346120\n",
      "Iteration 106, loss = 0.36351935\n",
      "Iteration 107, loss = 0.36282612\n",
      "Iteration 108, loss = 0.36304588\n",
      "Iteration 109, loss = 0.36279985\n",
      "Iteration 110, loss = 0.36248010\n",
      "Iteration 111, loss = 0.36157465\n",
      "Iteration 112, loss = 0.36107994\n",
      "Iteration 113, loss = 0.36137769\n",
      "Iteration 114, loss = 0.36030066\n",
      "Iteration 115, loss = 0.36119285\n",
      "Iteration 116, loss = 0.36067400\n",
      "Iteration 117, loss = 0.35979796\n",
      "Iteration 118, loss = 0.35924282\n",
      "Iteration 119, loss = 0.35938555\n",
      "Iteration 120, loss = 0.35847790\n",
      "Iteration 121, loss = 0.35966275\n",
      "Iteration 122, loss = 0.35739803\n",
      "Iteration 123, loss = 0.35708212\n",
      "Iteration 124, loss = 0.35953045\n",
      "Iteration 125, loss = 0.35795568\n",
      "Iteration 126, loss = 0.35722234\n",
      "Iteration 127, loss = 0.35668858\n",
      "Iteration 128, loss = 0.35546891\n",
      "Iteration 129, loss = 0.35564866\n",
      "Iteration 130, loss = 0.35624592\n",
      "Iteration 131, loss = 0.35607498\n",
      "Iteration 132, loss = 0.35561518\n",
      "Iteration 133, loss = 0.35530301\n",
      "Iteration 134, loss = 0.35527166\n",
      "Iteration 135, loss = 0.35356946\n",
      "Iteration 136, loss = 0.35349914\n",
      "Iteration 137, loss = 0.35386332\n",
      "Iteration 138, loss = 0.35376835\n",
      "Iteration 139, loss = 0.35382555\n",
      "Iteration 140, loss = 0.35369792\n",
      "Iteration 141, loss = 0.35361450\n",
      "Iteration 142, loss = 0.35197899\n",
      "Iteration 143, loss = 0.35336487\n",
      "Iteration 144, loss = 0.35121081\n",
      "Iteration 145, loss = 0.35156579\n",
      "Iteration 146, loss = 0.35116790\n",
      "Iteration 147, loss = 0.35164731\n",
      "Iteration 148, loss = 0.35006601\n",
      "Iteration 149, loss = 0.35256069\n",
      "Iteration 150, loss = 0.35217681\n",
      "Iteration 151, loss = 0.35090344\n",
      "Iteration 152, loss = 0.35085993\n",
      "Iteration 153, loss = 0.35050130\n",
      "Iteration 154, loss = 0.34922561\n",
      "Iteration 155, loss = 0.34986718\n",
      "Iteration 156, loss = 0.34956887\n",
      "Iteration 157, loss = 0.34897226\n",
      "Iteration 158, loss = 0.34926152\n",
      "Iteration 159, loss = 0.35069023\n",
      "Iteration 160, loss = 0.34907343\n",
      "Iteration 161, loss = 0.34926968\n",
      "Iteration 162, loss = 0.34770721\n",
      "Iteration 163, loss = 0.34724411\n",
      "Iteration 164, loss = 0.34739201\n",
      "Iteration 165, loss = 0.34821050\n",
      "Iteration 166, loss = 0.34870647\n",
      "Iteration 167, loss = 0.34680244\n",
      "Iteration 168, loss = 0.34647682\n",
      "Iteration 169, loss = 0.34617839\n",
      "Iteration 170, loss = 0.34538604\n",
      "Iteration 171, loss = 0.34771914\n",
      "Iteration 172, loss = 0.34691953\n",
      "Iteration 173, loss = 0.34515711\n",
      "Iteration 174, loss = 0.34702566\n",
      "Iteration 175, loss = 0.34514199\n",
      "Iteration 176, loss = 0.34567513\n",
      "Iteration 177, loss = 0.34580388\n",
      "Iteration 178, loss = 0.34557034\n",
      "Iteration 179, loss = 0.34435102\n",
      "Iteration 180, loss = 0.34489146\n",
      "Iteration 181, loss = 0.34469105\n",
      "Iteration 182, loss = 0.34370436\n",
      "Iteration 183, loss = 0.34466143\n",
      "Iteration 184, loss = 0.34456342\n",
      "Iteration 185, loss = 0.34458069\n",
      "Iteration 186, loss = 0.34367989\n",
      "Iteration 187, loss = 0.34343536\n",
      "Iteration 188, loss = 0.34328918\n",
      "Iteration 189, loss = 0.34409718\n",
      "Iteration 190, loss = 0.34287918\n",
      "Iteration 191, loss = 0.34291158\n",
      "Iteration 192, loss = 0.34250208\n",
      "Iteration 193, loss = 0.34243530\n",
      "Iteration 194, loss = 0.34229175\n",
      "Iteration 195, loss = 0.34311256\n",
      "Iteration 196, loss = 0.34218217\n",
      "Iteration 197, loss = 0.34285748\n",
      "Iteration 198, loss = 0.34137843\n",
      "Iteration 199, loss = 0.34126575\n",
      "Iteration 200, loss = 0.34103670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.53246833\n",
      "Iteration 2, loss = 0.51114867\n",
      "Iteration 3, loss = 0.50327591\n",
      "Iteration 4, loss = 0.49686578\n",
      "Iteration 5, loss = 0.49067563\n",
      "Iteration 6, loss = 0.48505916\n",
      "Iteration 7, loss = 0.47955946\n",
      "Iteration 8, loss = 0.47574683\n",
      "Iteration 9, loss = 0.47088946\n",
      "Iteration 10, loss = 0.46695216\n",
      "Iteration 11, loss = 0.46303741\n",
      "Iteration 12, loss = 0.45982963\n",
      "Iteration 13, loss = 0.45687406\n",
      "Iteration 14, loss = 0.45345136\n",
      "Iteration 15, loss = 0.45028402\n",
      "Iteration 16, loss = 0.44755247\n",
      "Iteration 17, loss = 0.44502100\n",
      "Iteration 18, loss = 0.44227255\n",
      "Iteration 19, loss = 0.44019411\n",
      "Iteration 20, loss = 0.43746743\n",
      "Iteration 21, loss = 0.43592668\n",
      "Iteration 22, loss = 0.43348603\n",
      "Iteration 23, loss = 0.43247695\n",
      "Iteration 24, loss = 0.42982899\n",
      "Iteration 25, loss = 0.42758760\n",
      "Iteration 26, loss = 0.42612959\n",
      "Iteration 27, loss = 0.42394841\n",
      "Iteration 28, loss = 0.42327077\n",
      "Iteration 29, loss = 0.42149542\n",
      "Iteration 30, loss = 0.42032441\n",
      "Iteration 31, loss = 0.41842649\n",
      "Iteration 32, loss = 0.41683946\n",
      "Iteration 33, loss = 0.41609299\n",
      "Iteration 34, loss = 0.41464935\n",
      "Iteration 35, loss = 0.41310316\n",
      "Iteration 36, loss = 0.41135266\n",
      "Iteration 37, loss = 0.41105265\n",
      "Iteration 38, loss = 0.40924363\n",
      "Iteration 39, loss = 0.40859562\n",
      "Iteration 40, loss = 0.40702123\n",
      "Iteration 41, loss = 0.40605953\n",
      "Iteration 42, loss = 0.40625872\n",
      "Iteration 43, loss = 0.40383019\n",
      "Iteration 44, loss = 0.40401605\n",
      "Iteration 45, loss = 0.40212979\n",
      "Iteration 46, loss = 0.40152446\n",
      "Iteration 47, loss = 0.40117096\n",
      "Iteration 48, loss = 0.39964576\n",
      "Iteration 49, loss = 0.39943866\n",
      "Iteration 50, loss = 0.39830171\n",
      "Iteration 51, loss = 0.39791250\n",
      "Iteration 52, loss = 0.39681371\n",
      "Iteration 53, loss = 0.39693105\n",
      "Iteration 54, loss = 0.39557431\n",
      "Iteration 55, loss = 0.39537462\n",
      "Iteration 56, loss = 0.39529237\n",
      "Iteration 57, loss = 0.39354990\n",
      "Iteration 58, loss = 0.39382811\n",
      "Iteration 59, loss = 0.39298971\n",
      "Iteration 60, loss = 0.39204055\n",
      "Iteration 61, loss = 0.39116191\n",
      "Iteration 62, loss = 0.39095723\n",
      "Iteration 63, loss = 0.38967105\n",
      "Iteration 64, loss = 0.38946624\n",
      "Iteration 65, loss = 0.39050181\n",
      "Iteration 66, loss = 0.38828456\n",
      "Iteration 67, loss = 0.38831096\n",
      "Iteration 68, loss = 0.38718358\n",
      "Iteration 69, loss = 0.38777586\n",
      "Iteration 70, loss = 0.38648776\n",
      "Iteration 71, loss = 0.38670384\n",
      "Iteration 72, loss = 0.38570183\n",
      "Iteration 73, loss = 0.38581968\n",
      "Iteration 74, loss = 0.38520734\n",
      "Iteration 75, loss = 0.38477514\n",
      "Iteration 76, loss = 0.38356758\n",
      "Iteration 77, loss = 0.38358220\n",
      "Iteration 78, loss = 0.38335425\n",
      "Iteration 79, loss = 0.38374694\n",
      "Iteration 80, loss = 0.38221563\n",
      "Iteration 81, loss = 0.38198154\n",
      "Iteration 82, loss = 0.38247633\n",
      "Iteration 83, loss = 0.38071878\n",
      "Iteration 84, loss = 0.38100970\n",
      "Iteration 85, loss = 0.37951388\n",
      "Iteration 86, loss = 0.38075140\n",
      "Iteration 87, loss = 0.37906479\n",
      "Iteration 88, loss = 0.37939427\n",
      "Iteration 89, loss = 0.37880830\n",
      "Iteration 90, loss = 0.37846891\n",
      "Iteration 91, loss = 0.37790479\n",
      "Iteration 92, loss = 0.37907755\n",
      "Iteration 93, loss = 0.37689239\n",
      "Iteration 94, loss = 0.37758922\n",
      "Iteration 95, loss = 0.37675920\n",
      "Iteration 96, loss = 0.37606153\n",
      "Iteration 97, loss = 0.37680056\n",
      "Iteration 98, loss = 0.37555964\n",
      "Iteration 99, loss = 0.37621653\n",
      "Iteration 100, loss = 0.37548455\n",
      "Iteration 101, loss = 0.37570650\n",
      "Iteration 102, loss = 0.37507212\n",
      "Iteration 103, loss = 0.37493056\n",
      "Iteration 104, loss = 0.37335817\n",
      "Iteration 105, loss = 0.37370289\n",
      "Iteration 106, loss = 0.37359646\n",
      "Iteration 107, loss = 0.37418363\n",
      "Iteration 108, loss = 0.37322859\n",
      "Iteration 109, loss = 0.37274219\n",
      "Iteration 110, loss = 0.37293874\n",
      "Iteration 111, loss = 0.37165431\n",
      "Iteration 112, loss = 0.37105311\n",
      "Iteration 113, loss = 0.37175616\n",
      "Iteration 114, loss = 0.37143116\n",
      "Iteration 115, loss = 0.37045193\n",
      "Iteration 116, loss = 0.37167757\n",
      "Iteration 117, loss = 0.37040873\n",
      "Iteration 118, loss = 0.37041761\n",
      "Iteration 119, loss = 0.37093603\n",
      "Iteration 120, loss = 0.36876106\n",
      "Iteration 121, loss = 0.36938707\n",
      "Iteration 122, loss = 0.36984883\n",
      "Iteration 123, loss = 0.36902996\n",
      "Iteration 124, loss = 0.36878184\n",
      "Iteration 125, loss = 0.36801912\n",
      "Iteration 126, loss = 0.36855859\n",
      "Iteration 127, loss = 0.36749988\n",
      "Iteration 128, loss = 0.36793626\n",
      "Iteration 129, loss = 0.36831871\n",
      "Iteration 130, loss = 0.36706799\n",
      "Iteration 131, loss = 0.36731010\n",
      "Iteration 132, loss = 0.36685423\n",
      "Iteration 133, loss = 0.36709973\n",
      "Iteration 134, loss = 0.36690705\n",
      "Iteration 135, loss = 0.36646605\n",
      "Iteration 136, loss = 0.36670098\n",
      "Iteration 137, loss = 0.36604433\n",
      "Iteration 138, loss = 0.36689465\n",
      "Iteration 139, loss = 0.36621380\n",
      "Iteration 140, loss = 0.36639296\n",
      "Iteration 141, loss = 0.36515721\n",
      "Iteration 142, loss = 0.36678440\n",
      "Iteration 143, loss = 0.36462513\n",
      "Iteration 144, loss = 0.36493902\n",
      "Iteration 145, loss = 0.36439588\n",
      "Iteration 146, loss = 0.36412805\n",
      "Iteration 147, loss = 0.36583819\n",
      "Iteration 148, loss = 0.36436644\n",
      "Iteration 149, loss = 0.36446330\n",
      "Iteration 150, loss = 0.36427631\n",
      "Iteration 151, loss = 0.36385527\n",
      "Iteration 152, loss = 0.36407030\n",
      "Iteration 153, loss = 0.36432338\n",
      "Iteration 154, loss = 0.36353267\n",
      "Iteration 155, loss = 0.36301267\n",
      "Iteration 156, loss = 0.36280429\n",
      "Iteration 157, loss = 0.36269006\n",
      "Iteration 158, loss = 0.36312532\n",
      "Iteration 159, loss = 0.36308809\n",
      "Iteration 160, loss = 0.36242784\n",
      "Iteration 161, loss = 0.36250263\n",
      "Iteration 162, loss = 0.36247185\n",
      "Iteration 163, loss = 0.36272915\n",
      "Iteration 164, loss = 0.36211428\n",
      "Iteration 165, loss = 0.36246383\n",
      "Iteration 166, loss = 0.36153839\n",
      "Iteration 167, loss = 0.36177630\n",
      "Iteration 168, loss = 0.36105564\n",
      "Iteration 169, loss = 0.36123501\n",
      "Iteration 170, loss = 0.36070292\n",
      "Iteration 171, loss = 0.36098460\n",
      "Iteration 172, loss = 0.36048359\n",
      "Iteration 173, loss = 0.36096846\n",
      "Iteration 174, loss = 0.36056886\n",
      "Iteration 175, loss = 0.36118118\n",
      "Iteration 176, loss = 0.35962835\n",
      "Iteration 177, loss = 0.35983384\n",
      "Iteration 178, loss = 0.35975292\n",
      "Iteration 179, loss = 0.36046707\n",
      "Iteration 180, loss = 0.36012638\n",
      "Iteration 181, loss = 0.35980341\n",
      "Iteration 182, loss = 0.35933366\n",
      "Iteration 183, loss = 0.35891242\n",
      "Iteration 184, loss = 0.35939634\n",
      "Iteration 185, loss = 0.35968565\n",
      "Iteration 186, loss = 0.35923515\n",
      "Iteration 187, loss = 0.35894573\n",
      "Iteration 188, loss = 0.35843804\n",
      "Iteration 189, loss = 0.35952771\n",
      "Iteration 190, loss = 0.35798591\n",
      "Iteration 191, loss = 0.35841984\n",
      "Iteration 192, loss = 0.35857749\n",
      "Iteration 193, loss = 0.35855096\n",
      "Iteration 194, loss = 0.35840123\n",
      "Iteration 195, loss = 0.35800730\n",
      "Iteration 196, loss = 0.35829122\n",
      "Iteration 197, loss = 0.35813943\n",
      "Iteration 198, loss = 0.35753356\n",
      "Iteration 199, loss = 0.35687216\n",
      "Iteration 200, loss = 0.35759847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the classification on GradientBoostingClassifier\n",
      "Running the classification on BaggingClassifier\n",
      "Running the classification on ExtraTreesClassifier\n",
      "Running the classification on DecisionTreeClassifier\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MLA Name</th>\n",
       "      <th>MLA Parameters</th>\n",
       "      <th>MLA Train Accuracy Mean</th>\n",
       "      <th>MLA Test Accuracy Mean</th>\n",
       "      <th>MLA Test Accuracy 3*STD</th>\n",
       "      <th>MLA Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "      <td>{'bootstrap': False, 'ccp_alpha': 0.0, 'class_...</td>\n",
       "      <td>0.995707</td>\n",
       "      <td>0.854065</td>\n",
       "      <td>0</td>\n",
       "      <td>32.3734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BaggingClassifier</td>\n",
       "      <td>{'base_estimator': None, 'bootstrap': True, 'b...</td>\n",
       "      <td>0.987767</td>\n",
       "      <td>0.81925</td>\n",
       "      <td>0</td>\n",
       "      <td>24.9842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>{'ccp_alpha': 0.0, 'class_weight': None, 'crit...</td>\n",
       "      <td>0.995707</td>\n",
       "      <td>0.781896</td>\n",
       "      <td>0</td>\n",
       "      <td>3.56244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.0001, 'batch...</td>\n",
       "      <td>0.855049</td>\n",
       "      <td>0.772681</td>\n",
       "      <td>0</td>\n",
       "      <td>180.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>{'ccp_alpha': 0.0, 'criterion': 'friedman_mse'...</td>\n",
       "      <td>0.715526</td>\n",
       "      <td>0.71452</td>\n",
       "      <td>0</td>\n",
       "      <td>81.1597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>{'C': 1.0, 'class_weight': None, 'dual': False...</td>\n",
       "      <td>0.707662</td>\n",
       "      <td>0.709482</td>\n",
       "      <td>0</td>\n",
       "      <td>7.77128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>{'priors': None, 'var_smoothing': 1e-09}</td>\n",
       "      <td>0.689979</td>\n",
       "      <td>0.690518</td>\n",
       "      <td>0</td>\n",
       "      <td>0.530611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>{'copy_X': True, 'fit_intercept': True, 'n_job...</td>\n",
       "      <td>0.260083</td>\n",
       "      <td>0.258506</td>\n",
       "      <td>0</td>\n",
       "      <td>1.47438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     MLA Name  \\\n",
       "6        ExtraTreesClassifier   \n",
       "5           BaggingClassifier   \n",
       "7      DecisionTreeClassifier   \n",
       "3               MLPClassifier   \n",
       "4  GradientBoostingClassifier   \n",
       "1          LogisticRegression   \n",
       "2                  GaussianNB   \n",
       "0            LinearRegression   \n",
       "\n",
       "                                      MLA Parameters MLA Train Accuracy Mean  \\\n",
       "6  {'bootstrap': False, 'ccp_alpha': 0.0, 'class_...                0.995707   \n",
       "5  {'base_estimator': None, 'bootstrap': True, 'b...                0.987767   \n",
       "7  {'ccp_alpha': 0.0, 'class_weight': None, 'crit...                0.995707   \n",
       "3  {'activation': 'relu', 'alpha': 0.0001, 'batch...                0.855049   \n",
       "4  {'ccp_alpha': 0.0, 'criterion': 'friedman_mse'...                0.715526   \n",
       "1  {'C': 1.0, 'class_weight': None, 'dual': False...                0.707662   \n",
       "2           {'priors': None, 'var_smoothing': 1e-09}                0.689979   \n",
       "0  {'copy_X': True, 'fit_intercept': True, 'n_job...                0.260083   \n",
       "\n",
       "  MLA Test Accuracy Mean MLA Test Accuracy 3*STD  MLA Time  \n",
       "6               0.854065                       0   32.3734  \n",
       "5                0.81925                       0   24.9842  \n",
       "7               0.781896                       0   3.56244  \n",
       "3               0.772681                       0   180.012  \n",
       "4                0.71452                       0   81.1597  \n",
       "1               0.709482                       0   7.77128  \n",
       "2               0.690518                       0  0.530611  \n",
       "0               0.258506                       0   1.47438  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#split dataset in cross-validation with this splitter class: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit\n",
    "#note: this is an alternative to train_test_split\n",
    "cv_split = ShuffleSplit(n_splits = 1, test_size = .20, train_size = .75, \\\n",
    "                                                random_state = 0 )\n",
    "                    # run model 10x with 60/30 split intentionally leaving out 10%\n",
    "\n",
    "#create table to compare MLA metrics\n",
    "MLA_columns = ['MLA Name', 'MLA Parameters','MLA Train Accuracy Mean', 'MLA Test Accuracy Mean', \\\n",
    "               'MLA Test Accuracy 3*STD' ,'MLA Time']\n",
    "MLA_compare = pd.DataFrame(columns = MLA_columns)\n",
    "\n",
    "#create table to compare MLA predictions\n",
    "MLA_predict = Y_all.copy()\n",
    "\n",
    "#index through MLA and save performance to table\n",
    "row_index = 0\n",
    "for alg in MLA:\n",
    "\n",
    "    #set name and parameters\n",
    "    MLA_name = alg.__class__.__name__\n",
    "    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n",
    "    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n",
    "    \n",
    "    print(\"Running the classification on %s\" %(MLA_name))\n",
    "    \n",
    "    #score model with cross validation: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate\n",
    "    cv_results = cross_validate(alg, X_all, Y_all, cv  = cv_split,return_train_score=True)\n",
    "\n",
    "    MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n",
    "    MLA_compare.loc[row_index, 'MLA Train Accuracy Mean'] = cv_results['train_score'].mean()\n",
    "    MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean()   \n",
    "    #if this is a non-bias random sample, then +/-3 standard deviations (std) from the mean, \n",
    "    #should statistically capture 99.7% of the subsets\n",
    "    MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std()*3   \n",
    "    #let's know the worst that can happen!\n",
    "    \n",
    "\n",
    "    #save MLA predictions - see section 6 for usage\n",
    "    alg.fit(X_all, Y_all)\n",
    "    MLA_predict[MLA_name] = alg.predict(X_all)\n",
    "    \n",
    "    row_index+=1\n",
    "\n",
    "    \n",
    "#print and sort table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort_values.html\n",
    "MLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace = True)\n",
    "MLA_compare\n",
    "#MLA_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   26.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    3.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE = 0.0041286658292829445\n",
      "Test MSE = 0.153969658245683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    1.0s finished\n"
     ]
    }
   ],
   "source": [
    "degree = 1\n",
    "model = RandomForestClassifier(n_estimators = 100,verbose=1)\n",
    "# model = KNeighborsClassifier(n_neighbors = 3)\n",
    "# model = MLPClassifier(verbose=1,max_iter=300,hidden_layer_sizes=(150,100,10,2,))\n",
    "# model = SVC(probability=True,verbose=1)\n",
    "# model = ExtraTreesClassifier()\n",
    "train_test(model,X_train,Y_train,X_test,Y_test,degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
